\chapter{Cognitive Models}

\section{Grounded Language Games}

The previous chapters introduced the physical setup and implementation of the robots. They explained the body, the situation and the behavioral part of the sytem. The only part of the implementation that still needs to be presented are the cognitive models that implement the categorization process and the lexicon formation. The categorization model is based on the discrimination game model proposed by \cite{steels:1996b} and the lexicon formation is based on the naming game model \cite{steels:1996a}. This chapter explains the models as they are evolved during this research.

\p
The previous chapter explained the interaction of the robots with their environment and each other. The interaction and segmentation as has been discussed form the contour of the physically grounded symbol system. Especially the segmentation is an important ingredient of the solution of the symbol grounding problem: it forms the first step towards invariancy. Invariance returns in the cognitive processes during the selection of elements. The two other recognized questions of the SGP, {\em discrimination} and {\em identification} \cite{harnad:1990}, are cognitively modelled. Discrimination is mainly modelled in the discrimination games, whereas the naming games model the identification.

Meaning can have many shapes. It may, for instance be an identification, an affordance or a discrimination. The choice here is made to model meaning as, like the name discrimination games would suggest, discrimination. Similarly, one could model {\em identification games} (see \cite{vogt:1999a} and chapter \ref{ch:followme}) or even other types of games. 

As \citeN{wittgenstein:1958} mentioned, the meaning of a language game lies in the use of the game. \citeN{steels:1997a} argued that language and meaning co-evolve. The structural coupling of an observation (or interaction) with a conceptual category and lexical element could be viewed as the meaning of something in a particular context. The emergence of such a coupling arises in a complex of steps. It starts with the interaction (perception and/or communication), second is the segmentation, third there is the categorization, then there is lexical association, possibly communication and finally there is the coupling to a feedback loop. Symbolically, the meaning can be grasped at different levels depending on the application. In this case the levels include: the categorization level, conceptualization level and the lexicalization level. In more complex systems where there is grammar, semantics can also be grasped in the grammatical structure of an expression. Futhermore, if the task is not communication, but for instance navigation or self-localization, even other levels may be identified.

Both models of categorization and lexicon formation are based on the same principles of interaction, adaptation and self-organization. Where evolution of categories is individual to an agent, the lexicon formation is a cultural evolution. Self-organization establishes as a result of the process of generation of categorical or lexical elements and the selection of those elements that appeared to be successful in the past.

\p
Although the basis of the discrimination games are similar to the model proposed in \cite{steels:1996b}, the method for representing the categories has changed significantly. Instead of using a binary tree for the representation a prototype model has been proposed. This is to investigate a different and possibly more plausible model of the discrimination game. The discrimination game model is presented in the next section.

Section \ref{s:cm:ng} introduces the naming game model. Several models are considered and have been introduced in several papers \cite{steels:1996a,steelskaplan:1998,vogt:1998c}. The models are introduced concise as they are described elsewhere more extensively, see e.g. \cite{steels:2000}.

Finally, this chapter discusses how the two models are coupled together and thus how structural couplings arise. The coupling of the two models is thought to be a key issue in bootstrapping a coherent system.

\section{Categorization}

\subsection{Discrimination Games}\label{s:cm:dg}

The model of the discrimination games was first proposed in \cite{steels:1996b}. The basis of the model has not changed since, but the implementation and precise details have been adjusted ever since. The first robot implementation of the model can be found in \cite{steelsvogt:1997} and \cite{vogt:1998a}. As mentioned the model is built on the same foundations of evolution as the naming game model. It is exploites a selectionist mechanism of generation and selection and results in an organization of categories that has the properties of a dynamical system.

Suppose that an agent observes a particular scene with sensors that can detect light coming from the outside world\footnote{An agent may also detect with sensors that inspect its internal state.}. Such an observation results in a set of raw sensory data that represents the scene. Depending on the sensors that the agent uses, the format of the data can be very differently. A robot with camera vision ususally gets some kind of bitmap representing $n \times m$ pixels. Each pixel can be described by a number referring to the color of that pixel and/or to its brightness or other features. As in most vision applications the vision routines needs to preprocess the raw data in order to find regions of interest in the observed scene. Specially designed filters (usually more than one) filter the raw data. From the filtered image the system can find particular regions of interest (or {\em segments} for short) describing these regions with certain features like {\bf position}, {\bf color} or {\bf shape}. The aim of our model is to categorize these segments.

The robots used in the experiments are equipped with low-level sensors, which can only detect light from the environment without spatial information, let alone discriminative information. In order to get a spatial and discriminative view of an environment, either the robot needs to have a spatially distributed array of sensors or the robot needs to move. Because of the physical limitations of the robots (and the SMBII in particular) it is opted to let the robots move. This way a higher resolution scan can be obtained from the environment.

Chapter \ref{ch:lg} discussed the perception and segmentation already in detail. So, assume that the agent has observed a set of segments, called the context $C=\{s_1,\ldots,s_n\}$, which has been preprocessed with a set of sensory channels $SC=\{sc_0,\ldots,sc_{N-1}\}$. Let a feature be the result of applying a sensory channel to an region of interest $R_m$ (see section \ref{s:lg:segmentation}), which can be described as $f_i=sc_i$-$V_i$, where $V_i=f_i({\bf s}_{m,i})$. Each segment $s_j$ can be described by a set of features: $s_j=\{f_0,\ldots,f_{N-1}\}$.

\p
Let $O=\{c_1,\ldots,c_M\}$ be the ontology of categories of the robot\footnote{Two notes has to be place here: (1) Note the different use of ontology in this context. Where it initially was used to refer to both the set of available categories, concepts and word-forms, it now refers to the available categories alone. (2) Recall that the categorization is an individual process, hence the reference made to {\em the} robot. No further comments are made on this issue in this section.}, where $c_k=(sc_{i,k},attr_k,\nu_k)$ is some relation from sensory channel $sc_{i,k} \in SC$ with one or more attributes $attr_k$ and a score $\nu_k$. The use of sensory channel $sc_{i,k}$ in this definition should not be confused with the function defined earlier, its use is to indicate the relation of $c_k$ with sensory channel $sc_i$. Category $c_k$ can only be related to a feature that has been calculated with sensory channel $sc_i$; it cannot be related to any other sensory channel. 

The category set $O$ may be empty and initially it is. New elements can be added to $O$ according to rules as they will be defined later.

Each segment $s_j$ can be related with a set of categories $\Lambda^{s_j} \subseteq O$. This set may be empty, but it may also yield several categories, depending on both the segment and the relation. How $\Lambda^{s_j}$ is constructed depends on the defintion of the attributes, which determine how a segment is related to a category. The attributes will be defined in subsequent subsections. For the moment assume that $\Lambda^{s_j}$ is just a set of categories.

Now a set of concepts of segment $s_j$ can be defined as follows:

\begin{eqnarray}
\Gamma^{s_j}=&\{\Gamma^{s_j}_p \mid (\Gamma^{s_j}_p \subseteq \Lambda^{s_j}) \wedge \forall (c_k, c_l \in \Gamma^{s_j}_p, k \neq l) : (sc_k \neq sc_l) \}
\end{eqnarray}

This means that concepts are subsets of categories of the segment. Each concept is a set of at least one category. If the concept has more than one category, then the different categories should relate to different sensory channels. Obviously each concept differs from other concepts. Note that the union of all concepts of a segment is equal to the set of categories for that segment, i.e. $\bigcup_p \Gamma^{s_j}_p = \Lambda^{s_j}$. If a segment has only one category, then the set of concepts has only one element and this concept is the category, i.e. $\mid\Lambda^{s_j}\mid=1 \Leftrightarrow \mid\Gamma^{s_j}\mid=1 \Leftrightarrow \Gamma^{s_j}= \{\Gamma^{s_j}_1\} = \{\Lambda^{s_j}\}$.

In a discrimination game the aim is to find distinctive concepts of a segment $s_j$. The set of distinctive concepts $D^{s_j}$ is defined as:

\begin{eqnarray}
D^{s_j}=\{\Gamma^{s_j}_{p} \in \Gamma^{s_j} \mid \forall (s \in C\setminus\{s_j\}):(\neg \exists \Gamma^{s}_q\in\Gamma^{s}: (\Gamma^{s_j}_p=\Gamma^{s}_q))\}
\end{eqnarray}

So, a concept $\Gamma^{s_j}_p$ is distinctive if and only if there is no other segment $s$ in the context that has a concept $\Gamma^{s}_q$ for which $\Gamma^{s_j}_p=\Gamma^s_q$. 

The discrimination game is a success if $D^{s_j} \neq \emptyset$. In this case several things may be done:

\begin{enumerate}
\item If the discrimination game is used as part of a language game, the set will be used in the naming part of the language game. According to the outcome of the language game, some of the scores that are part of the attributes, which may vary per experiment, may be updated.
\item The categories of the concept may adapt to the observation, so that the concepts evolve dynamically and adaptive towards the observations of the robot.
\item Individual category scores and concept scores may be adapted.
\end{enumerate}

If the discrimination game is a failure, i.e. $D^{s_j}=\emptyset$, then the ontology $O$ must be adapted. For this, depending on the methods for representing categories, one or more new categories may be invented. Details of some of these methods are discussed in the next section.

\begin{figure}
\centerline{\psfig{figure=discr_games//schema_dg.eps,width=11.4cm}}
\caption{A schematic overview of the discrimination games. The dashed arrow between the segments and ontology is used during the adaptation of the ontology.[wat meer over zeggen?]}
\end{figure}

In this section the formal model of the discrimination games were introduced. A schematic overview of this model is shown in figure \ref{dg:schema_dg}. The whole process from perception until conceptualization constist of the the following processes: perception, pre-processing, segmentation, categorization, conceptualization, discriminaton and adaptation. All these processes can be modified and tested by variants of the general model as presented. We now will discuss some of the variants that have been investigated.

\subsection{Binary Tree Method}

\begin{figure}
\centerline{\psfig{figure=discr_games//binary_tree.eps,width=11.4cm}}
\caption{Categories represented as binary trees. Every sensory channel (like WL, ML and IR) is associated with a category tree. The root node of the tree is sensitive to whole range of the sensory channel ($[0,1]$) in this example. The tree is incrementally constructed during the evolution of discrimination games. Every time the discrimination game fails, two new categories are constructed by splitting one category.}
\label{dg:binary_tree}
\end{figure}

In the original paper by Luc Steels the categories were constructed as a binary tree \cite{steels:1996b}. The root of such a tree is sensitive to the complete range of a sensory channel. The children nodes of the root sensitive to the upper half and lower half of a sensory channel. These nodes in the tree could in turn be exanded by two children, each sensitive to one half of the range of the parent node, etc. (see fig. \ref{dg:binary_tree}). Every node that has children is said to be {\em refined}.

This method was also applied in the first implementation of the robotic language games \cite{steelsvogt:1997}\cite{vogt:1998a} and has been reimplemented in a later version of the experiment [moet nog gedaan worden]. See chapter .. for a discussion of the results.

More concretely the method is a variant of the model defined in section \ref{dg:the_model}, where the $attr_k$ of category $c_k$ is defined more precisely. The attribute of the category is defined in this method as an interval: $attr_k=\langle v_{k,0},v_{k,1}]$, so the category $c_k$ can now be defined as $c_k=(sc_{i,k},\langle v_{k,0},v_{k,1}],\nu_k)$. A segment $s_j=\{f_0,\ldots,f_{N-1}\}$ can now be related to a set of categories $\Lambda^{s_j}=\{c_0,\ldots,c_M\}$:

\begin{eqnarray}
\Lambda^{s_j}=&\{c_k=(sc_{i,k},\langle v_{k,0},v{k,1}],\nu_k)\; \mid (c_k \in O) \wedge\\
& \exists (f_i=sc_i\mbox{-}V_i \in s_j): (v_{k,0} < V_i \leq v_{k,1})\} \nonumber
\end{eqnarray}

\p
So, in words the category set consists of categories for which the value each feature of the segment is in the interval of the category.

The only thing that still has to be defined is how new categories can be constructed. If a discrimination game fails in finding distinctive categories for the topic $t$, then the robot may construct a new category. This happens according to the following rules:

\begin{itemize}
\item {\bf If} there are still sensory channels for which no category has been introduced, then select from these channels one arbitrary sensory channel $sc_i$ for which $V_i>L$ if $sc_i$-$V_i \in t$, where $L$ is the lower-bound of the sensory channel's range (usually $L=0$). Add to the ontology $O$ the new category $c=(sc_i,\langle L,U],\nu)$, where $U$ is the upperbound of the sensory channel's range and the category score $\nu=0$.
\item {\bf Else} choose an arbitrary category $c_k \in \Lambda^t$ that is not refined yet. The refinement of $c_k=(sc_{i,k},\langle v_{k,0},v_{k,1}],\nu)$ yields two new categories $c'=(sc_i,\langle v_{k,0},v_{k,\frac{1}{2}}],\nu')$ and $c''=(sc_i,\langle v_{k,\frac{1}{2}},v_{k,1}],\nu'')$, where $v_{k,\frac{1}{2}}=v_{k,0}+\frac{1}{2}\cdot(v_{k,1}-v_{k,0})$ and $\nu'=\nu''=0$. These categories are then added to the ontology $O$.
\end{itemize}

This way the ontology grows through constructing binary trees which refine the sensory channels.

The method of incorporating binary trees has both its advantages and its disadvantages. First, the categories a robot can construct are similar in the respect that they are generated similarly. This makes comparisons of the ontology and lexicon easier. Secondly, it makes a search through the category trees easier than in the prototype method (which will be discussed in the next subsection). The main disadvantage is that it is biologically less plausible. [leg uit].

\subsection{The Prototype Method}

The main core of the categories in the experiments reported in this thesis use what we might call {\em prototypes}. The categorization model that uses prototypes as the representation of categories could be called the {\em prototype method} \cite{dejongvogt:1998} and \cite{vogt:1998c}.


The prototype method uses a different represenation of categories than in the original model proposed by \cite{steels:1996b}. This represeantation is influenced by the prototype theory of \cite{rosch} and more particular by the ideas presented in \cite{lakoff:1987} and \cite{barsalou:1999}. Instead of a discrete representation like binary trees, prototypes are centered around dynamically changing regions of the sensory space. Prototypes are coupled to the environment of observations and tend to evolve towards particular regions of interest. A {\em region of interest} can be defined as a region of the sensory space that has been activated often. The {\em centre} of the region of interest evolves to those values the sensory space that correspond to the prototype, i.e. the centre tends to shift towards the average sensor value of those values that have been categorized with the prototype. Rosch [cite] has called these centres {\em basic-level categories}, but since Rosch's theory involves a more higher level description of categories (e.g. {\em animal} or {\em bird}) another notation will be used here.

So how are prototypes represented in the model? Te aim was to keep the concept of hierarchical layering of categories, so that agents still could generalize and specialize over the categories. Another goal was to preserve the idea that the categories were grounded from observations and thus represent past experiences of the agent. 

\begin{figure}
\centering
\subfigure[]{\psfig{figure=discr_games//proto1.eps,width=11.4cm}}
\subfigure[]{\psfig{figure=discr_games//proto2.eps,width=11.4cm}}
\end{figure}

\begin{figure}
\centering
\subfigure[]{\psfig{figure=discr_games//proto3.eps,width=11.4cm}}
\caption{The figures (a), (b) and (c) each correspond to a sensory channel. These figures show the structure and selection of hierarchical prototypes. On the right-hand side of the arrows the hierarchical structures are shown. From left to right each layer can hide more prototypes, shown here as dots (compare each prototype with a node of the binary tree shown in the previous section).
Suppose that a robot observed a segment with the following features $\{$WL-0.725, ML-0.65, IR-0.10$\}$. Then each feature (shown on the left-hand side of each figure) can be related to the categories that are shown as black dots on the right-hand side. At each layer the prototype that has its value closest to the observation is selected (their names are listed the bottom of each figure). If the discrimination game fails, a new prototype may be introduced as shown in figure (a). A layer with free space is chosen and a new prototype is added with the same value as the observation. This process is constrained by some rules, see the text for details.}
\label{dg:prototree}
\end{figure}

Figure \ref{dg:prototree} shows a schematic hierarchy of prototypes grouped in sensory channels. The figure also shows how observations can be categorized and how new categories can be generated. A {\em hierarchy of prototypes} is a layered structure that consists of a (possibly) increasing amount of categories\footnote{The terms prototypes and categories shall be used as synonyms. In fact, a prototype is a sort of a category, when another form of category is meant, this will be expressed explicitely, otherwise a category is a prototype and vice versa.}. The amount of prototypes $N(\lambda)$ on layer $\lambda$ is constrained by the following rule: $N(\lambda)={N_0}^\lambda$, where $N_0$ is the order with which the hierarchy can grow. Furthermore, it is assumed that no two categories can have the same value at the same layer.

\p
More formally, we can define a prototype $c_k$ by filling in the attributes $attr_k$ as the tuplet $attr_k = (v_k, \lambda_k)$, where $v_k$ is the value of the category and $\lambda_k$ is its layer, so the category can be defined as: $c_k = (sc_{i,k}, v_k, \lambda_k, \nu_k)$. Let $d_{i,k}=|V_i-v_k|$ be the absolute distance between value $V_i$ of feature $f_i$ and value $v_k$ of category $c_k$, then a segment $s_j=\{f_0,\ldots,f_{N-1}\}$ can now be related to a set of categories $\Lambda^{s_j}=\{c_0,\ldots,c_M\}$:

\begin{eqnarray}
\Lambda^{s_j}=&\{c_k=(sc_{i,k},v_k,\lambda_k,\nu_k)\; \mid (c_k \in O) \wedge (\forall (f_i=sc_i\mbox{-}V_i \in s_j):\\ \nonumber
& \forall (c_k,c_l \in O \wedge k \neq l \wedge \lambda_k = \lambda_l): (d_{i,k} \leq d_{i,l}))\}
\end{eqnarray}


When a discrimination game fails, a new category will be introduced:

\begin{itemize}
\item {\bf If} there are still sensory channels for which no category has been introduced, then select from these channels one arbitrary sensory channel $sc_i$. Add to the ontology $O$ the new category $c=(sc_i,v,\lambda,\nu)$, where $v=V_i$ is the observed value of $sc_i$, layer $\lambda=0$ and the category score $\nu=0$.
\item {\bf Else} choose an arbitrary sensory channel $sc_i$. Find for this sensory channel the first layer $\lambda$ for which $N'(\lambda) < N_0^\lambda$ and $\neg \exists c = (sc_i,v,\lambda,\nu): v=V_i$. Then add category $c'=(sc_i,V_i,\lambda,\nu)$ to ontology $O$ with $\nu=0$.
\end{itemize}

If a discrimination game succeeds, one distinctive concept $\Gamma^t_p \in \Gamma^t$ will be selected for use in the communication. As mentioned in section \ref{s:dg:model}, categories may be adapted in several ways. One possible adaptation may be that prototypes shift into the direction of the actual perception of a segment, thus facilitating what might be compared to a {\em prototype effect} \cite{rosch}. The value $v$ of a prototypical category $c=(sc_i,v,\lambda,\nu) \in \Gamma^t_p$ shifts towards the perception of segment $t$ as follows:

\begin{eqnarray}
v'=v+\epsilon \cdot (V_i-v)
\end{eqnarray}

\p
where $V_i$ is the value of sensory channel $i$ of segment $t$, $v'$ is the new value. Stepsize $\epsilon$ can be varried, but is taken to be $0.1$ throughout the experiments, unless stated otherwise.


\subsection{The Adaptive Subspace Method}

Another method that can be used to relate and generate categories using the discrimination game has been called the {\em adaptive subspace method} \cite{dejongvogt:1998}. Although the method is not used in the experiments reported here, it is discussed briefly to indicate the strength of the discrimination game as a method rather than the representations by which the categories are stored.

\begin{figure}[t]
\centerline{\psfig{figure=discr_games//boom2.eps,width=11.4cm}}
\caption{\label{fig_boom}Refining a feature set using the Adaptive Subspace method. Taken from (De Jong and Vogt, 1998).}
\end{figure}

\begin{quote}
The Adaptive Subspace method is based on the principle that different sets of sensory inputs should only be treated as different if this distinction is meaningful. Which distinctions are meaningful is determined by the application. In the case of discrimination games, a distinction should be made if and only if this increases the ability of the agent to discriminate the topic from the other objects, in one or more of the discrimination games it has played. This method results from research on generalization, an important issue in machine learning. An overview is beyond the scope of this paper, but for interesting contributions on generalization, including other subspace methods, see \cite{landelius:mastersthesis,%Sutton:96,
murao:96,McCallum:phd}. In this paper, orthogonal splits were used; an interesting variation would be to allow non-orthogonal splits as well. Oja discusses subspace methods where, unlike here, subspaces have a lower dimensionality than the original data \cite{oja:83}. 

When a discrimination game is played, the agent determines for each object it has observed, by which feature set it is covered\footnote{A feature set is called a category.}. If the feature set of the topic covers no other object, the agent can discriminate the topic from the other objects, and the game is a success. When a game fails, the feature set of the topic is investigated to see whether it should be refined. Refining a feature set amounts to splitting one of the ranges in half and replacing the existing feature set with two new feature sets, one for each half. This is illustrated in figure \ref{fig_boom}. 

\cite[My footnote]{dejongvogt:1998}
\end{quote}

One important difference with the binary tree method and the prototype method(section \ref{s:dg:bintree}) is that the subspace method incorporates all dimensions of the sensory channel space, whereas the other methods use a subset of the n-dimensional space (i.e. it does not necessarily use all dimensions to discriminate). The way the subspaces are refined are similar to the binary tree method in that a sensory space is split into two equal halves in a particular dimension. The adaptation of the subspaces, however, is constrained stronger than the two other methods. A subspace is only adapted when statistically the split is thought to increase improvement, whereas the other methods adapt their structures more blindly.


\subsubsection{Scores}

The categories are all listed together with a score $\nu_k$. This score is used to keep track of the effectiveness to discriminate of category $c_k$. It is updated after a discrimination game as follows:

\begin{eqnarray}
\nu_k'=\alpha \cdot \nu_k + \beta \cdot S
\end{eqnarray}

where
\[
S = \left \{ \begin{array}{ll} 1 & \mbox{if } c_k \in D^{s_j}\\
 0 & \mbox{otherwise}
\end{array}
\right. \]

Besides the category score, a meaning score is used in some of experiments. The meaning score $\mu_{\Gamma}$ for distinctive feature set (or meaning) $\Gamma$ takes four different parameters into account: (1) The average feature score $\nu_{\Gamma}$ of the feature scores $\nu$ of features that constitute the meaning, see section 3. (2) A success score $\phi_{\Gamma}$ for the distinctive feature set $\Gamma$ in relation to the effectiveness in the lexicon. (3) A measure $\kappa_{\Gamma}$ that indicates the average depth of the distinctive feature set in the features' hierarchy. And (4) a measure $\chi_{\Gamma}$ indicating the size of the distinctive feature set. All measures are bounded between 0 and 1. The measures are calculated as follows (for $\nu$, see eq. \ref{featurescore}):

\begin{eqnarray}
\displaystyle
\phi_{\Gamma}=\phi'_{\Gamma} +  \tau \cdot S
\end{eqnarray}
\noindent
where
\[
\left \{
\begin{array}{rl}
S=1 & \mbox{ if } \Gamma \mbox{ is used in the expression}\\
S=-1 & \mbox{ if } \Gamma \mbox{ is not used in the expression}
\end{array}
\right.
\label{eta}
\]

\noindent
and $\phi'_{\Gamma}$ is the original value. This parameter is calculated at the end of every language game. Depth value $\kappa_{\Gamma}$ is calculated as follows:

\begin{eqnarray}
\displaystyle
\kappa_{\Gamma}=\frac{1}{5} \cdot (5 - \beta)
\end{eqnarray}

\noindent
where $\beta$ is the average depth in the hierarchy. Set-size measure $\chi_{\Gamma}$ is defined as:

\begin{eqnarray}
\displaystyle
\chi_{\Gamma}=\frac{1}{|\Gamma|}
\end{eqnarray}

\noindent
These two latter measures are instantly calculated when needed.

We can now define the meaning score $\mu_\Gamma$ as:

\begin{eqnarray}
\displaystyle
\mu_{\Gamma}=\frac{1}{4}\cdot (\nu_{\Gamma}+\phi_{\Gamma}+\kappa_{\Gamma}+\chi_{\Gamma})
\end{eqnarray}

\noindent
The value $\mu_{\Gamma}$ is normalized in order not have too much influence of the meaning on the naming.


\subsection{Summary}

This section presented the categorization model by which the grounding problem is tackled. The categorization is based on the discrimination game model that tries to identify symbolic representations that relate to the topic, but not to any other segment that has been observed in the language game situation.

Several methods have been introduced to define the structure of the symbolic representations. The methods implement a binary tree, a hierarchy of prototypes and an adaptive subspace. It will be shown in subsequent chapters that the representation does not influence the grounding process very much. This implies that the strength of the categorization lies in the model of the discrimination model.

Past effectiveness of categories and concepts are stored in scores that can be used to select the categories and concepts. How this is done will be explained in section \ref{s:selection}. The next section introduces the naming game model.

\section{Lexicon Formation}

The lexicon formation is based on the naming game model introduced by Luc \citeN{steels:1996a}. The naming game implements the communication between two agents that try to name objects they 'observed' in their environment. One of the agents plays the role of the speaker in the naming game and chooses a topic from the objects in the context. It searches its lexicon for an element of the meaning matches the meaning of the topic. The associated word-form is 'expressed' and in turn, the hearer tries to understand the expression. The hearer does so by searching its own lexicon for an element of which the word-form matches the expression. If there exist such an element, the hearer compares the associated meaning(s) with the meaning of the topic. If there is a match, the naming game is successful. Otherwise there is a failure. According to the outcome of the game the lexicon will be adapted. In the original naming game model the speaker informed the hearer what the topic was prior to the communication. In later models the hearer was less reliably informed about the topic, or even not at all. Then the hearer's aim was to guess the topic using the information it received. This type of game has been called the {\em guessing game} \cite{steelskaplan:1999}.

\p
So, each agent builds up a lexicon. How does the lexicon look like? It consists of elements of word-meaning associations. Each word-meaning association {\bf WM} is a tuple of a word-form $F$, a meaning $\Gamma_p$, an association score $\sigma$ and possibly a counter of use $u$ and success $s$. So, the lexicon $L$ can be defined as:

\begin{eqnarray}
L=\{{\bf \mbox{WM}}_0, \ldots, {\bf \mbox{WM}}_N\}
\end{eqnarray}

\noindent
where $N=0,1,2,\ldots,M$, bounded by $M<\infty$ and word-meaning ${\bf \mbox{WM}}_i=(F_i,\Gamma_{p,i},\sigma_i[,u_i,s_i])$. Initially $L=\emptyset$, so $N=0$. The lexicon is constructed during the experiment. The word-form $F$ is an arbitrary string of characters. 

\p
The adaptation of the lexicon is done by word-invention, word-adoption, construction of word-meaning associations and the adaptation of scores. During the experiment where thousands of games are being played the word-meaning associations that have been successful in the past (i.e. their scores are high) tend to be used more often than nonsuccessful word-meaning associations. This way a more or less coherent communication system emerges. How the process of selection and adaption is implemented will be explained in the remainder of this section.


\subsection{The Speaker's Production}\label{s:cm:production}


When the speaker categorized the topic $t$, which yielded a nonempty set of distinctive concepts $D^t$, the speaker will try to name one of these concepts. Which concept is selected may depend on several criteria and the selection method used. Two methods have been implemented:

\begin{enumerate}
\item {\bf Lazy search:} The speaker orders the concepts in linear order of decreasing meaning score $\mu$. Then it tries to encode these meanings one by one until a matching association has been found.

When tying to encode $\Gamma^t_k$, the speaker selects that word-meaning association ${\bf \mbox{WM}}_i \in L$ for which:

\begin{eqnarray}
\Gamma_{i}=\Gamma^t_k\nonumber
\end{eqnarray}
\noindent
{\em and}
\begin{eqnarray}
\sigma_i = \max_{\forall j:\Gamma_{j}=\Gamma^t_k} (\sigma_j)
\label{e:select}
\end{eqnarray}

\noindent
Or in words: The speaker selects that word-meaning association for the meaning matches the selected meaning and for which the association score is highest. Note that one word-form can be associated with more than one meaning, but one meaning can also be associated with more than one word-form. Thus the lexicon can both hide ambiguity as synonymy.

\item {\bf Complete search:} The hearer tries to encode all the disctincive concepts gathered in $D^t$ and selects that concept $\Gamma^t_k$ that has the highest association score. So, eq. \ref{e:select} holds for again, but now all possible $k$ are explored whereas in the previous method, the search stops when a solution has been found.
\end{enumerate}

\p
When the speaker thus produced a word-form that it can express, the hearer takes over trying to understand the speaker.

\subsection{The Hearer's Understanding}\label{s:cm:understanding}


The hearer's understanding is more complex. It first of all depends on what knowledge the hearer receives about the topic prior to the linguistic communication\footnote{Recall that the linguistic communication is the transfer of the word-form from the speaker to the hearer. If this would happen on-board of the robot, the radio-link would be the medium. Since the cognition is processed off-board, the medium is the PC. All communication by which no word-form is transfered, is called non-linguistic communication. Pointing, for instance is non-linguistic communication. So, is the communication that the robots need to use for synchornizing their states during the execution of a language game.}. Three basic principles of prior topic knowledge have been explored: (1) absolute topic knowledge, (2) indicative topic knowledge and (3) no topic knowledge. Method (1) reduces to the naming game model described in \cite{steels:1996a} and used in \cite{steelsvogt:1997}. Method (2) has been proposed in \cite{steelskaplan:1998} and has been explored in \cite{vogt:1998b,vogt:1998c}. Method (3) has been first explored in \cite{vogt:1998c} and has been called the {\em guessing game} \cite{steelskaplan:1999}.

The problem is that a word-form is usually associated with more than one meanings. Sometimes these meanings have been categorized for different segments. Especially when a 'young' word-form has been used. In the early period of a word-form it gets associated with possibly many meanings before an effective association emerges. Until that period the word-form has to be disambiguated. Selection criteria models the disambiguation using a matrix in which more than one hypothesis can be considered by the hearer.

All three principles of topic knowledge can be defined using a scheme in which the hearer constructs a desicion matrix of the word-meaning associations that matches the received word-form (see table \ref{t:matrix}). In every row there is one word-meaning association. The first column indicates to which segment the association belongs. The second column lists the meaning of the associations that matches one of the distinctive concepts found by the relevant segment. The third column indicates the likelihood that the relevant segment is the topic, the so-called {\em topic score} $\varepsilon$. Note that only those segments for which the topic score is larger than zero. The fourth column gives the meaning scores $\mu$ of the meaning. Column five gives the association score $\sigma$ and the final column gives the sum $\Sigma$ of columns three, four and five.

\begin{table}
\begin{center}
\begin{tabular}{||c|c|c|c|c|c||}
\hline \hline
$S$ & $\Gamma$ & $\varepsilon$ & $\mu$ & $\sigma$ & $\Sigma$\\
\hline \hline
$s1$ & $\Gamma^{s1}_1$ & 0.1 & 0.11 & 0.04 & 0.25\\
\cline{2-6}
& $\Gamma^{s1}_2$ & 0.1 & 0.00 & 0.31 & 0.41\\
\hline
$s2$ & $\Gamma^{s2}_1$ & 0.7 & 0.47 & 0.95 & 2.12\\
\cline{2-6}
& $\Gamma^{s2}_2$ & 0.7 & 0.53 & 0.76 & 1.99\\
\hline
$s3$ & $\Gamma^{s3}_1$ & 0.6 & 0.60 & 0.25 & 1.45\\
\cline{2-6}
& $\Gamma^{s3}_2$ & 0.6 & 0.05 & 0.34 & 0.99\\
\cline{2-6}
& $\Gamma^{s3}_3$ & 0.6 & 0.45 & 0.10 & 1.15\\
\hline \hline
\end{tabular}
\caption{A possible decision matrix of the hearer. The matrix is an example produced to clarify the decision process; it is not taken from an actual experiment.}
\label{t:matrix}
\end{center}
\end{table}

The hearer selects the word-meaning association for which the sum $\Sigma$ is maximum. The segment that belongs to this association is taken to be the topic.

The different principles of prior topic knowledge are modelled by different methods for calculating the topic score $\varepsilon_s$ for segment $s$. This is done as follows:

\begin{description}
\item {\bf Indicative topic knowledge a}\footnote{Recall that the segment does not appear in the matrix when $\varepsilon=0$.}:
\begin{eqnarray}
\varepsilon_{s} = \frac{1}{1+({\frac{d_s}{\alpha}})^2}
\label{e:topicscore}
\end{eqnarray}
\noindent
where $d_s$ is the distance between the angle at which segment $s$ is observed and the angle at which the speaker pointed at, and $\alpha$ is a tolerance factor that sizes the {\em focus of attention} (see also section \ref{s:lg:pointing}).

\item {\bf Indicative topic knowledge b}: The system calculates the cross-correlation between each segment $s_h$ of the hearer and the speaker's topic $t_s$:

\begin{eqnarray}
\displaystyle
C_{s_h,t_s}=\frac{\sum_{i=0}^{N-1} V_{t_s,i} \cdot V_{s_h,i}}
{\sqrt{\sum_{i=0}^{N-1} V^2_{t_s,i} \cdot \sum_{i=0}^{N-1} V^2_{s_h,i}}}
\label{e:correlation}
\end{eqnarray}

\noindent
where $V_{s,i}$ is the value of feature $f_i$ of segment $s$. The topic score for segment $s$ is now defined as:

\begin{eqnarray}
\varepsilon_s=C_{s,t_s}
\end{eqnarray}

\noindent
Of course this method for calculating the topic score is very unlikely to exist in nature. Agents usually are not capable inspecting the internal state of other agents. However, to increase the reliability of the topic information, pointing here is simulated by {\em intenal inspection} implemented by calculating cross-correlations.
\item {\bf Absolute topic knowledge a}:
\begin{eqnarray}\varepsilon_s = \left \{ \begin{array}{rl}
1 & \mbox{if}\;\varepsilon_s' = \max_S (\varepsilon_s')\\
0 & \mbox{otherwise}
\end{array}
\right.
\end{eqnarray}

\noindent
where $\varepsilon_s'$ is the topic score as calculated in eq. \ref{e:topicscore}.

\item {\bf Absolute topic knowledge b:} 
\begin{eqnarray}\varepsilon_s = \left \{ \begin{array}{rl}
1 & \mbox{if}\; C_{s,t} = \max_S (C_{s,t})\\
0 & \mbox{otherwise}
\end{array}
\right.
\end{eqnarray}

\noindent
where $C_{s,t}$ is calculated with eq. \ref{e:correlation}.
\item {\bf No topic knowledge}:
\begin{eqnarray}
\forall i,j: \varepsilon_i=\varepsilon_j>0
\end{eqnarray}
\end{description}

\noindent
Note that item (1) does not exactly reduce to the method introduced in \cite{steels:1996a}. This happens only when the meaning score $\mu_k=0$ for all word-meaning associationss ${\bf \mbox{WM}}_k$.

\p
So, the hearer looks for a word-meaning association that best fits the expressed word-form. Now the naming game can be evaluated on its success. This evaluation will serve as the extremely important feedback of the language (and naming) game.

\subsection{Feedback}


Obviously an agent has to know whether the language game it is participating is successful in order to learn a language: it needs {\em feedback}. How the feedback is accomplished is an important issue, because (1) the literature has different views on the issue and (2) the implementation from the engineering point of view is difficult. Remember that the system is unsupervised and no intervention of the experimenter is required or even allowed. The robots have to provide the feedback themselves, or the feedback has to be found in the environment.

\p
The literature on situated cognition stresses the importance of feedback as a structural coupling between an organism and its environment \cite{clancey:1997}. The way this feedback is provided is an important issue of debate (see e.g. \cite{bowerman:1988}). Is the feedback provided {\em positive} or {\em negative}, {\em explicit} or {\em implicit}? Questions that cannot be answered immediately and need some investigation. The nativist approach takes that the child can get feedback from its environment when testing hypotheses. As \citeN{bowerman:1988} puts it when discussing the ideas of \citeN{braine:1971}:

\begin{quote}
Hypothesis testing requires feedback about the correctness of predictions, pointed out Braine. In particular, it requires evidence not only about what {\em is} an instance of is to be learned [...], but also what is {\em not} an instance.
\end{quote}\cite[p. 74]{bowerman:1988}

Although the argument applied directly to the nativist theory and to the acquisition of grammar, it also applies to the model proposed here, since it uses negative evidence as feedback as well. And

\begin{quote}
[a]fter reviewing the available data (e.g. \cite{brownhanlon:1970}), Braine concluded that negative evidence is rare in the input to children; moreover, children appear to be relative impervious to what little correction they do receive.
\end{quote}
\cite[p. 75]{bowerman:1988}

\todo{Verder uitwerken theorie feedback en no negative feedback, vergeet niet phychologische lit. Theorie misschien bespreken in hfdstk theorie}

It has been implemented by means of pointing, 'eye-contact', (relative) internal inspection and using bayesian statistics. Physical pointing has been explained in chapter \ref{ch:lg}. The hearer points in the direction of the topic the hearer understood, and if this is close to the speaker's intended topic, the language game is successful. The method of 'eye contact' is that the hearer signals success when it found an association matching the speaker's expression, so the language game is successful whether or not both agents communicated about the same segment. With internal inspection, the feedback inspects the internal state of both robots and the language game is a success when the segments have highest correlation. When the agents use bayesian statistics, they count co-occurences of word and meaning to select WM associations. Below each method is explained in more detail. The experiments investigate different means of feedback, combined with different means of prior topic knowledge, chapter \ref{xxx} will report on the results.

Usually an agent should determine the success of an interaction itself, which probably would lead to different results. For simplicity reasons however, it is assumed that both agents participating in a language game receive the same feedback. Only in the bayesian case the agents do determine the success of a game themselves.

\begin{description}
\item {\bf Pointing:} The hearer points to the topic it derived in the same way the speaker points. A success score $\epsilon$ is calculated from the speaker's point of view as what might be called a confidentiality factor. The calculation of $\epsilon$ uses the same formula as eq. \ref{e:topicscore}:

\begin{eqnarray}
\epsilon=\frac{1}{1+({\frac{d_{t_s,t_h}}{\alpha}})^2}
\label{e:successscore}
\end{eqnarray}
\noindent
where $d_{t_s,t_h}$ is the difference between the observation angle of the speaker's topic $t_s$ and pointed angle of the hearer's topic $t_h$. The value of $\alpha$ determines the size of the {\em focus of attention}, the region around the pointed angle in which $t_s$ should be for the language game to be successful. See also figure \ref{f:lg:} on page \pageref{f:lg:}. Since $d_{t_s,t_h} \leq \alpha \Rightarrow S \geq 0.5$, the language game is successful when $\epsilon \geq 0.5$. It should be obvious that the pointing is carried out off-board. 
\item {\bf Relative internal inspection:} The cross-correlation between the speaker's topic and the hearer's topic is calculated. If $\epsilon \geq \Theta$, the language game is a success. So, $\epsilon$ is calculated as:

\begin{eqnarray}
\displaystyle
\epsilon=\frac{\sum_{i=0}^{N-1} V_{t_s,i} \cdot V_{t_h,i}}
{\sqrt{\sum_{i=0}^{N-1} V^2_{t_s,i} \cdot \sum_{i=0}^{N-1} V^2_{t_h,i}}}
\end{eqnarray}

\noindent
where $V_{t,i}$ is the value of feature $f_i$ of segment $t$.
\item {\bf Absolute internal inspection:} 

\begin{eqnarray}
\epsilon=\left \{ \begin{array}{rl}
1 & \mbox{if}\;C_{t_s,t_h} \max_{r \in S_h} (C_{t_s,r})\\
0 & \mbox{otherwise}
\end{array} \right.
\end{eqnarray}

\noindent
where cross-correlation $C_{t_s,t_h}$ is calculated according eq. \ref{e:correlation} and $S_h$ is the set of segments observed by the hearer.

\item {\bf Eye contact:} When the hearer understood the speaker, i.e. the hearer had a matching association, the language game is successful. This does not necessarily mean that $t_s=t_h$.

\item {\bf Bayesian statistics:} The feedback in this method is provided by the context, categorization and naming itself. Co-occurences are counted to be enable the calculation of the probability of word-form $w$ given category $c$ ($P(w|c)$) and the probability of category $c$ given word-form $w$ ($P(c|w)$). These probabilities are used to select word-meaning associations. Since this method influences the language game in more ways, the method is described in more detail along the experimental results in section \ref{s:xx:bayes}.

\end{description}

The lexicon is adapted according to the outcome of the game as will be explained in the next subsection.

\subsection{Adaptation}

There are several posible outcomes of a language game. The game can already fail during the categorization. This will put pressure to the agent to increase its repertoire of categories as explained in section \ref{s:cm:dg}. Another failure could be due to the fact that the speaker does not have a word-form association matched to category to be named. In this case the agent can invent a new word-form to associate with the category. If the hearer does not understand the speaker, this means that it does not have a proper word-meaning association. The expressed word-form can be adopted and associated with one or more categories. When there is a mismatch in meaning and when the language game was a success, the association scores are updated.

\begin{description}
\item {\bf No lexical entry speaker:} In this case, the speaker may invent a new word-form as an arbitrary string of characters. It does so with a creation probability $P_s$ that is kept low to slow down the word-form creation rate, which decreases ambiguity (see section \ref{s:chXX:XX}).

\item {\bf No lexical entry hearer:} The hearer now may adopt the word-form from the hearer to associate it with a segment of which it has a certain likelihood (the topic score $\varepsilon_t$) that this segment may be the topic. This is the case when 

\begin{eqnarray}
\varepsilon_t = \max_S (\varepsilon)
\label{e:adopt}
\end{eqnarray}

\noindent
If there are more than one segments for which eq. \ref{e:adopt} holds, then one segment is selected at random. The meaning of the selected segment is then associated with a (relatively high) adoption probability $P_h$.

\item {\bf Mismatch in meaning:} In the case that both robots selected a WM association, but when the topic did not coincide, at least according to their own evaluation, the robots decrease the association strength $\sigma_{\mbox{WM}}$ of the used association $\mbox{WM}$:

\begin{eqnarray}
\sigma := \eta \cdot \sigma
\label{e:adapt1}
\end{eqnarray}

\n
where $\eta$ is the {\em learning rate}. In some experiments the hearer also adopts the word-form with another segment.

\item {\bf Communicative success:} In the case that the language game was successful, the used association is strenthened while association scores of other WM associations are inhibited. If $\mbox{WM}' = (F',\Gamma',\sigma') \in L$ and $\mbox{WM} = (F,\Gamma,\sigma) \in L$ are WM associations, then the scores are updated as follows:

\begin{eqnarray}
\sigma := \eta \cdot \sigma + (1 - \eta) \cdot S
\label{e:adapt2}
\end{eqnarray}

\n
where

\begin{eqnarray}
S = \left \{ \begin{array}{rl} 1 & \mbox{if}\; \mbox{WM}' = \mbox{WM} \nonumber\\
0 & \mbox{if}\; (\mbox{WM}' \neq \mbox{WM}) \wedge ((F' = F) \vee (\Gamma' = \Gamma))\nonumber \end{array} \right.
\end{eqnarray}

\end{description}

The adaptation scheme thus allows generation and selection. Generation is part of the adaptation, whereas the selection is influenced by the excitation and inhibition of the association scores. The seemingly effective associations are excited and the ineffective ones are inhibited. The ontology is spread throught the community of speakers by the word-adoption of the hearer.

\p
Other adaptation methods varying the adaptation of the scores have been investigated as well. The above equations \ref{e:adapt1} and \ref{e:adapt2} calculate the running average of the successfulness of a WM association. These scores can also be adapted keeping a counter of use and success. Eq. \ref{e:adapt1} becomes:

\begin{eqnarray}
u := u + 1\nonumber\\
\sigma = \frac{s}{u}
\end{eqnarray}

\n
and eq. \ref{e:adapt2} becomes:

\begin{eqnarray}
u' := u' + 1\nonumber
\end{eqnarray}
\begin{eqnarray}
s' := \left \{ \begin{array}{ll} 
s'+1 & \mbox{if}\;\mbox{WM}'=\mbox{WM}\\
s' & \mbox{if}\; (\mbox{WM}' \neq \mbox{WM})\\
& \; \wedge ((F' = F) \vee (\Gamma' = \Gamma))\nonumber\\ \end{array} \right.
\end{eqnarray}
\begin{eqnarray}
\sigma' = \frac{s'}{u'}\nonumber
\end{eqnarray}

A third method uses the following equations replacing eq. \ref{e:adapt1} and \ref{e:adapt2} resp.:

\begin{eqnarray}
\sigma := \min ((\sigma-\delta \cdot \xi),0)
\end{eqnarray}

\n
and

\begin{eqnarray}
\sigma' := \left \{ \begin{array}{ll}
\max ((\sigma' + \delta \cdot \xi),1) & \mbox{if}\;\mbox{WM}' = \mbox{WM}\\
\min ((\sigma-\delta \cdot \xi),0) & \mbox{if}\; (\mbox{WM}' \neq \mbox{WM})\\ 
& \;\wedge ((F' = F) \vee (\Gamma' = \Gamma))\end{array} \right.
\end{eqnarray}

\n
where $\delta$ is some constant, $\xi$ is a confidential factor. The value of $\xi$ is varied across different experiments: either $\xi=\epsilon$ or $\xi=1$.

The first method score adaptation will be referred to as the {\em walking average method} and this is the default method. This method is used unless otherwise indicated. The second method will be called the {\em success vs. use method} and has been originally been introduced in \cite{steels:1996a}. The final method will be referred to as the {\em incremental method}.

\section{Coupling Categorization and Naming}\label{s:coupling}

The preceding part of this chapter presented the categorization/conceptualization and lexicalization in detail. In principle the categorization could serve other problems than language acquisition. For example in a robot-task like landmark navigation. In this work, however, the categorization is coupled to the naming game to serve language acquisition. Depending on the task, the method for categorization can be varied. And depending on the type of language game (e.g. guessing, naming, follow me or imitation game) the selection process of concepts to be used in the communication can vary. In section \ref{s:production} the selection process for the speaker is explained and section \ref{s:understanding} explains the hearer's selection. The naming is the next step in coupling the language game. Feedback is coupled with both the communication and some external factors that indicate the effectiveness of the game. The first steps in the coupling process however, is made in the agent's perception and segmentation.

\begin{figure}
\psfig{figure=discr_games//coupling.eps,width=11.4cm}
\caption{A schematic view of the structural coupling in the language game. See the text for details.}
\label{f:coupling}
\end{figure}

\p
Figure \ref{f:coupling} shows a schematic view of the complete structural coupling of a language game. Above the division line is the speaker and the hearer is shown below. The speaker observes a referent (R). Note that it normally detects more, but for clarity only one referent is drawn. Perception ({\it P}) of results in some signal or {\em percept} (P) that corresponds to the detection of R. Segmentation ({\it S}) reduces the amount of sensory data in a relatively low dimensional description of the perceived signal. These processes have been described extensively in the previous chapter. 

Each observed segment (S) is categorized ({\it C}) yielding a set of categories ($\Lambda$). This process has already been presented above in figure \ref{f:prototree}. The next step in the process is the conceptualization ({\it C'}) of these categories, which results in a set of concepts ($\Gamma$). $\Gamma$ consists of all possible configurations of categories that are in $\Lambda$, with the exception of those that bear more than one category related to a particular sensory channel. Each concept is shown in a rectangular box that represents a matrix representation in relation to $\Lambda$. There are concepts with only one category and the concepts are built up to all configurations of the same dimension of the robot's sensory apparatus. Note that in the experiments there are actually four sensory channels, where there are only three shown. Furthermmore, note that not all possible configurations are shown due to space limitations. Discrimination ({\it D}) compares all the concepts of $\Gamma$ with all the concepts conceptualized from other segments in the context (and which are not shown in the figure). Those concepts that are unique to the segment of the topic are listed in the set of distinctive concepts (D).

The speaker then tries to lexicalize ({\it L}) the distinctive concepts by looking for association with word-forms in the lexicon (L). Depending on the method used for selection, the speaker selects that association that fits best the selection criteria. The selected distinctive concept and association are shown in bold and the selected lexical element is shaded. Finally this word-form is 'uttered' ({\it U}) by the speaker.

\p
The bottom half of figure \ref{f:coupling} shows how the hearer constructs a coupling between the speaker's utterance and its own perception. More or less parallel to the speaker the hearer perceives the referents, segments the percepts, categorizes and conceptualizes these percepts. Note that the figure shows the perception and conceptualization of two referents, whereas for the speaker only one is shown. This is done to indicate that the hearer may consider more segments to be the topic\footnote{The reader may notice that the percept of the upper referent the hearer perceives is the same as the speaker's percept. Obviously this is not the case in a real situation (see section \ref{s:lg:perception}).}.

When the hearer receives the utterance, it tries to discriminate each observed segment. This is done, because the discrimination takes a lot of computation time and there is no need to discriminate when there is no linguistic communication. 

When the hearer was able to discriminate (i.e. $D \neq \emptyset$), then it tries to decode the received word-form. It searches its lexicon (which typically is different than the speaker's lexicon) for associated meanings (M). These meanings may coincide with some of the distinctive concepts the hearer conceptualized, but this is not alway the case (see the rightmost meaning). Note that the meanings have the same structure as the concepts and actually they are the same. The matching ({\it M}) meaning with the highest score according to the rules explained in section \ref{s:understanding}, determines the topic that the hearer {\em identifies}. The structural coupling becomes complete by the feedback that the system provides. Identification emerges as the language game is successful.

\p
Although for clarifying reasons figure \ref{f:coupling} shows copies of the conceptual structures, these are not necessarily there (and they are not). 
The concepts listed in $\Gamma$ could be configured by excitations of the structure shown in $\Lambda$. In turn the distinctive concepts could be excited even more and of them would have excited association couplings to lexical entries. The structural coupling that has the highest association value, at least according to the selection criteria, wins the competition between the other couplings, see figure \ref{f:excitation}. As can be seen in figure \ref{f:excitation} (b) the architecture shows many similarities with a neuronal structure. However, the growth and selection of connections are much more plastic than most contemporary neural network architectures.

\begin{figure}[t]
\subfigure[]{\psfig{figure=discr_games//excitation.eps,width=11.4cm}}\\
\subfigure[]{\psfig{figure=discr_games//excitation1.eps,width=11.4cm}}
\caption{Reducing the copies made in figure \ref{f:coupling} makes the coupling more direct as they are shown in the semiotic diagram. Figure (a) skips the distinctive concepts and (b) skips also the set of concepts (only the upper half is shown). Of cource they are still there, but the connections are more direct and thus more biologically plausible. In case (b) it should be noted that the possible necessity of more dimensional associations could be regulated by thresholding the association scores.}
\label{f:excitation}
\end{figure}

So, putting together rather simple mechanisms as introduced in the last two chapters, a complex system may emerge that can use a symbolic communication system as structural couplings between two agents and their environment. The coupling is complete only within the context in which the language game is situated and when the language game is successful. All three recognized issue of the symbol grounding problem (signals-to-symbols, discrimination and identification) are now solved. The agents construct symbolic representations for analog signals they can perceive. The discrimination game obviously solves the problem of discrimination. Identification is only solved when the language game is successful\footnote{In other applications, like for instance navigation identification succeeds when the task succeeds.}. If either discrimination or identification fails, the agent(s) can adapt their ontology so, they may be successful in the future.

\p
Now raises the long expected question: Are the experiments successful? Before the results are presented, the answer is given. Yes, the experiments are successful, but only to a certain extend. Although the robots are very successful in the signals-to-symbols and discrimination problems, they have great difficulty to construct a stable communication system and thus have difficulties in identification. However, it will be shown that in principle, these problems could be overcome, and they are!
