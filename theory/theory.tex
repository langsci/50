
\chapter{Theoretical Motivation}\label{ch:theory}

\section{Introduction}

To study the origins and evolution of language using machines, a clear theoretical framework has to be designed. One needs to draw boundaries on what to study and how. Where should one start? How should one start? What does one want to show? Which theoretical framework should be taken as the guide to show how mobile robots can construct a shared communication system? All questions that need to be asked before one starts to develop a robotic system that simulates the origins of language. In general, one should ask such questions always before one investigates a certain phenomenon.

The main goal of this thesis is to show that mobile robots can develop a shared communication system from (cultural) interactions with its environment through self-organization. It is a part of ongoing research to show that language can be viewed as a complex dynamic adaptive system, cf. \cite{steels:1997b}. The dynamical systems approach should be seen within the {\em situated embodiment} approach to cognition. Before the theoretical background of this approach is explained, it is argued why the traditional (and still very popular) Chomskian approach is wrong.

\index{embodiment}
\index{embodiment!situated -|see{situated!embodiment}}
\index{situated!embodiment}
\index{situated!cogntition}

The approach that is taken here is that of what recently has been called {\em situated embodiment} \cite{zlatev:1997}. This approach is the merging of a collection of approaches that have been introduced in the cognitive sciences from the 1970s and have been reintroduced in Artificial Intelligence in the past two decades. The term {\em situated embodiment} merges {\em situated cognition} \cite{clancey:1997} with {\em embodiment} \cite{lakoff:1987}. Each of these dialectic views are in turn influenced by a collection of theories for which we find an overview in this chapter.

It is not the first time such an approach has been taken to investigate cognition (and language) in mobile robots, but as far as known it is the first time that the approach is taken to investigate {\bf the evolution and origins of language on mobile robots}.

This chapter is organized as follows: The next section discusses the classical approach to language. Section \ref{s:theory:sgp} discusses the Symbol Grounding Problem and categorization. Selection and self-organization is the subject of section \ref{s:theory:select}. Some background issues on early human language development is given in section \ref{s:theory:acquisition}. Other related work is reviewed in sections \ref{s:theory:otherwork} and \ref{s:theory:th}. The chapter concludes with a summarizing discussion and a statement of interest for the rest of this thesis.

\section{Classical Approaches}\label{s:theory:classic}

In the 1960s and 1970s some influential classical {\em cognitivist} theories on the origins and evolution of language and meaning arose. This section discusses the theories introduced by Noam Chomsky \citeyear{chomsky:1956,chomsky:1980} and Jerry Fodor \citeyear{fodor:1975}. They are discussed because they take an extreme and opposite standpoint about the nature of language than is pursued here. The discussion starts with a rough overview of the theories proposed by Chomsky and Fodor, together with some main critiques. Some alternative views are discussed in subsequent sections.

\subsection{Generative Linguistics}\label{s:theory:chomsky}

\index{Chomsky, Noam}
From 1956, when Noam Chomsky gave a lecture in front of the Institute of Radio Engineers at MIT, the linguistic community was turned upside down \cite{chomsky:1956}. Chomsky proposed a revolutionary vision about how language should be studied. Instead of investigating how language users perform in a language community, Chomsky proposed to study the {\em competence} of language. He said that linguists should study how a language formally can be described. The field of linguistics that investigates Chomsky's theory is called {\em Generative Linguistics}.

\index{Universal Grammar}
From the poverty of the stimulus argument\footnote{The poverty of the stimulus argument is the observation that children get less input from its linguistic environment than they are able to produce in later life.}, Chomsky argued that children must have innate knowledge of language, otherwise it would be impossible to learn language. So, a {\em Universal Grammar} was hypothesized that could explain all universal tendencies one can observe in human languages. Humans are, according to this view, born with this Universal Grammar (UG) encoded in the human genome. Parameters of the UG are set during the child's interaction with its language environment. Thus allowing the child to learn the language of its environment. This language gene causes a human to develop a brain structure in which the language can be acquired, the so-called \index{language!acquisition device}{\em Language Acquisition Device} (LAD). Noam Chomsky hardly ever explained how this LAD could have been evolved biologically and when he did, he argued that this LAD was just a spandrel of human brain evolution (see e.g. the discussion in \cite{pinkerbloom:1990,dennett:1995}).

So, according to the Chomskians all human languages have universal structures that are genetically determined. This would be fine if one could find such structures. Unfortunately, generative linguists have not found any non-trivial universal structure that holds across every language. [voorbeeld??/citations]

Another thing that makes Chomsky's theory less plausible is the postulation of the LAD as an innate brain structure. Of course there is an innate structure in our brain that is involved with language learning, but this structure is not likely to contain a essential linguistic structures like a UG. But how, for instance can the LAD explain the possibility that a child who has a lesion in a brain part of the LAD acquires language just as good as {\em normal} healthy children do?

The idea of investigating the competence of languages is particularly good when a language needs to be described or when one wants to understand how natural languages are structured formally. However, the way language is used by its users differs significantly from the formal description of a language. Hence it is more interesting to investigate the {\em performance} of language users and trying to answer questions why and how is language {\em used}?

\subsection{Language of Thought}\label{s:theory:fodor}

\index{Fodor, Jerry}
\index{language!of thought}
In line with Chomsky, Jerry Fodor proposed a similar hypothesis about concepts and meaning \cite{fodor:1975}. He hypothesizes a {\em Language of Thought} (LoT) to explain why humans tend to think in a {\em mental} language rather than in natural language alone. Fodor's theory consists of three radical statements.

First, Fodor argues that concepts can be described by symbols that represent {\em propositions} towards which attitudes (like {\em beliefs}, {\em desires}) can be attributed. Fodor calls these symbols {\em propositional attitudes}. If $P$ is a proposition, then the phrase ``I belief that $P$'' is a propositional attitude. According to Fodor, all mental states can be described as propositional attitudes, so a mental state is a belief or desire {\em about} something. This {\em something}, however is a proposition, which according to Fodor is {\em in the head}. But mental states should be about something that is in the real world. That is the essence of the symbol grounding problem. The propositions are symbol structures that are represented in the brain, sometimes called {\em mental representations}. In addition, the brain consists of rules that describe how these representations can be manipulated. The LoT, according to Fodor, is constituted by symbols which can be manipulated by applying existing rules. Fodor further argues that the LoT is innate, and thus resembles Chomsky's UG very well.

The second part of Fodor's theory is about concept learning. Concepts are in this Computational Theory of Mind (as Fodor's theory sometimes is called) constructed from a set of propositions. The LoT (and with that concepts) can, however, not be learned according to Fodor, who denies:

\index{Fodor, Jerry}
\index{language!of thought}

\begin{quote}
[r]oughly, that one can learn a language whose expressive power is greater than that of a language that one already knows. Less roughly, that one can learn a language whose predicates express extensions not expressible by those of a previously available representational system. Still less roughly, that one can learn a language whose predicates express extensions not expressible by predicates of the representational system {\em whose employment mediates the learning}. \cite[p. 86, Fodor's italics]{fodor:1975}
\end{quote}

\n
According to this, the process of concept learning is the testing of hypotheses that are already available at birth.

The third part of Fodor's theory is about perception. According to Fodor, perception is again the formulating and testing of hypotheses, which are already available to the agent.

\p
So, Fodor argues that, since one cannot learn a concept if one does not have the conceptual building blocks of this concept. And since perception needs such building blocks as well, concept learning does not exist and therefore concepts must be innate. This is a remarkable finding, since it roughly implies that all that we know is actual innate knowledge. Fodor called this innate inner language ``Mentalese''. It must be clear that it is impossible to have such a language. As Patricia S. Churchland puts it:

\begin{quote}
[The Mentalese hypothesis] entails the ostensibly new concepts evolving in the course of scientific innovation - concepts such as atom, force field, quark, electrical charge, and gene - are lying ready-made in the language of thought, even of a prehistoric hunter-gatherer... The concepts of modern science are defined in terms of of the theories that embed them, not in terms of of a set of ``primitive conceptual atoms,'' whatever those may be. \cite[p. 389]{p.s.churchland:1986}
\end{quote}

\n
Although it must be clear by now that this Computational Theory of Mind is essentially wrong, there are still many scientist who adheres to this theory and not the least many AI researchers. This is not surprising, since the theory tries to model cognition computationally, which of course is admirable since computers are computational devices. It will be shown however that Fodor's Computational Theory of Mind and Chomsky's Generative Linguistics are not necessary for concept and language learning.

So now that it is clear that humans are not born with inherited concepts, the question rises how concepts are acquired? 

\section{The Symbol Grounding Problem}\label{s:theory:sgp}

\subsection{Physical Symbol Systems}\label{s:theory:pss}

\index{physical!symbol system}

Probably the most influential pioneers of Artificial Intelligence are Herbert Simon and Allen Newell (who coincidentally introduced their first pioneering work at the same symposium for the IRE as Chomsky did \cite{newellsimon:1956}). Newell and Simon introduced the notion of a physical symbol system (PSS) as any physical system that have some constraints (see below) and that could be called intelligent \cite{newell:1980}. The notion of a PSS is important in this thesis, since what has been build  are to some extend PSS (or at least they should be).

\bigskip\noindent
The notion of a physical symbol system has been introduced to (roughly) define what a should be if it is said to be intelligent. In general it has to be a symbol system that can act in a physical environment and that is physically grounded. Newell and Simon recognized the following constraints that are put on intelligent systems (at least on the human mind) \cite[p.19]{newell:1990}:

\begin{enumerate}
\item Behave flexibly as a function of the environment.
\item Exhibit adaptive (rational, goal-oriented) behavior.
\item Operate in real time.
\item Operate in a rich, complex, detail environment.
\begin{itemize}
\item Perceive an immense amount of changing detail.
\item Use vast amounts of knowledge.
\item Control a motor system of many degrees of freedom.
\end{itemize}
\item Use symbols and abstractions.
\item Use language, both natural and artificial.
\item Learn from the environment and from experience.
\item Acquire capabilities through development.
\item Operate autonomously, but within a social community.
\item Be self-aware and have a sense of self.
\item Be realizable as a neural system.
\item Be constructable by an embryological growth process.
\item Arise through evolution.
\end{enumerate}

Obviously these are all features one can recognize as part of human cognition. It is often referred to as the closest 'definition' of intelligence. A physical symbol system should be capable of doing all the above. Although it would be tempting, the system presented in this thesis is not a full fledged PSS in that it is not capable of performing all the items enumerated. The system however closely resembles a PSS. Newell and Simon introduced the physical symbol system hypothesis, which states that humans may be considered to be physical symbol systems. Schematically, a physical symbol system is illustrated in figure \ref{f:pss}.

\begin{figure}[t]
\centerline{\psfig{figure=theory//pss.eps,width=11.4cm}}
\caption{Schematic interpretation of a physical symbol system. The cognition part takes the function of memory, symbol access and operations that a PSS could take. It should be noted that this PSS is directly situated in its environment. Although it may not be very clear, but Newell and Simon do position the PSS in its environment.\index{physical!symbol system}}
\end{figure}
\label{f:pss}

\p
\index{physical!symbol system}The PSS hypothesis is essentially not wrong, although the approach of Newell and Simon is classical. They implemented their ideas in a cognitive architecture called SOAR. The problem of SOAR is that the symbols they propose are rather static structures that have assigned their meaning implicitly. The symbols' meanings are given by the designer and have no direct relation with the world.

A big question for philosophers, linguists, psychologists, neuropsychologists and computer scientists is: How do symbols stand for something in the real world? How do symbols relate to the environment of an agent? Or, maybe even better, how do symbols get their {\em meaning}? This problem has been discussed by many scientists and will be the central discussion of this thesis. In philosophy, this problem is called {\em intentionality} \cite{bretano:1874}, whereas in AI it is often referred to as the {\em symbol grounding problem} \cite{harnad:1990}. The rest of this section will discuss the symbol grounding problem.

\subsection{Understanding Chinese}

Already for more than a century philosophers ask themselves how is it possible that we seem to think in terms of symbols which are {\em about} something that is in the real world. So, if one manipulates symbols as a mental process, one could ask what is the symbol (manipulation) about?\footnote{Although this thesis argues against classical cogntitivism, discussing cognition in terms of (manipulating) symbols can be justified as will be shown later.} Most explanations in the literature are however in terms of symbols that again are about something as in folk-psychology intentionality is often explained in terms of beliefs, desires etc. This however won't suffice if one wants to build a machine that can use and manipulate symbols which are about something that exists in the real world.

\bigskip\noindent
\index{Chinese Room}
\index{Searle, John}
This problem was made clear excellently by John R. Searle with a gedanken experiment called the {\em Chinese Room} \cite{searle:1980}. In this experiment, Searle considers himself standing in a room in which there is a large data bank of Chinese symbols and a set of rules how to manipulate these symbols. Searle, while in the room receives symbols that represent a Chinese expression. Searle, who does not know any Chinese, manipulates these symbols according to the rules such that he can output (other) Chinese symbols as if it was responding correctly in a human like way (only in Chinese). Moreover, this room passes the Turing test for speaking and understanding Chinese.

Searle claims that this room cannot understand Chinese because he himself does not. Therefore it is impossible to build a computer program that can have mental states and thus being what Searle calls a {\em strong AI}\footnote{It is not the purpose of this thesis to show that computer programs can have mental states, but to show that symbols in a robot can be about something.}. It is because Searle inside the room does not know what the Chinese symbols are about that Searle concludes that the room does not understand Chinese. Searle argues with a logical structure by using some of the following premises\cite[p. 39]{searle:1984}:

\index{Chinese Room}
\index{Searle, John}

\begin{enumerate}
\item Brains cause minds.
\item Syntax is not sufficient for semantics.
\item Computer programs are entirely defined by their formal, or syntactical, structure.
\item Minds have mental contents; specifically, they have semantic contents.
\end{enumerate}

Searle draws his conclusions from these premises in a correct logical deduction, but for instance premise (1) seems incomplete. This premise is drawn from Searle's observation that:

\begin{quote}
(A)ll mental phenomena ... are caused by processes going on in the brain. \cite[p. 18]{searle:1984}.
\end{quote}

One could argue in favor of this, but Searle does not mention what causes these brain processes. Besides metabolic and other biological processes that are ongoing in the brain, brain processes are caused by sensory stimulation (and maybe even by {\em sensorimotor} activity as a whole). So, (at least some) mental phenomena are at least to some extend caused by an agent's\footnote{I refer to an agent when I am talking about an autonomous agent in general, be it a human, animal, machine or something else.} interaction with its environment.

Premise (3) states that computer programs are entirely defined by their formal structure, which is correct. Only Searle equates formal with syntactical, which is correct when syntactic means something like {\em manipulating symbols according to the rules of the structure}. The appearance of {\em symbols} in this definition is crucial, since they are by definition about something. If the symbols in computer programs are about something, the programs are also defined by their semantic structure.

Although Searle does not discuss this, it may be well possible that he makes another big mistake in assuming that he (the central processing unit) is the part where all mental phenomena should come together. An assumption which is debatable (see e.g. \cite{dennett:1991,edelman:1992}), it is more likely that consciousness is more distributed. But it is not the purpose here to explain consciousness, instead the question is how are symbols about the world; the Chinese room is presented to make clear what the problem is and how philosophers deal with it.

Obviously Searle's Chinese room argument found a lot of opposition in the cognitive science community. The critique presented here is in line with what has been called the {\em system's reply} and to a certain extend the {\em robot's reply}\footnote{See for instance the critiques that appeared in the open peer commentary of Searle's 1980 article in the Behavioral and Brain Sciences.}. The system's reply holds that it is not the system who does not understand Chinese, but it is {\em Searle} who does not. The system as a whole does, since it passed the Turing test. 

\index{Chinese Room}
\index{Searle, John}
The robot's reply goes as follows: The Chinese room as a system does not have any other input than the Chinese symbols. So the system is a very unlikely cognitive agent. Humans have perceptual systems that receive much more information than only linguistic information. Humans perceive visual, tactile, auditory, olfactory and many other information; the Chinese room does, as it seems, not. So, what if we build a device that has such sensors and like humans has motor capacities? Could such a system with Searle inside understand Chinese?

According to Searle (in his answer to both the system's as robot's reply \cite{searle:1984}) his argument still holds. He argues that both the system's reply and the robot's reply do not solve the syntax vs. semantics argument (premise (2)). But the mistake that Searle makes is that premise (3) does not hold, thus making premise (2) redundant. Furthermore, in relation to the robot's reply Searle fails to notice the fact that brain processes are (partly) caused by sensory input and thus are mental phenomena indirectly caused by sensory stimulation.

And even if Searle's arguments are right, in his answer to the robot's reply he fails to understand that a robot is actually a {\em machine}. It is not just a computer that runs a computer program. And as Searle keeps on stressing:

\begin{quote}
'Could a machine think?' Well, in one sense, of course, we are all machines. (...) [In the] sense in which a machine is just {\em a physical system which is capable of performing certain kinds of operations} in that sense we are all machines, and we can think. So, trivially there are machines that can think. \cite[p. 35, my italics]{searle:1984}
\end{quote}

The reason why the phrase ``a physical system which is capable of performing certain kinds of operations'' is emphasized is because it is exactly that what a robot is. A robot is more than a computer that runs a computer program.

A last point that is made in this section is that Searle does not speak about development. Could Searle learn to understand Chinese if it was in the room from its birth and that he learned to interpret and manipulate the symbols that were presented to him? It is strange that a distinguished philosopher like Searle does not understand that it is possible to develop computer programs which can learn.

\bigskip\noindent
\index{Chinese Room}
\index{Searle, John}
The Chinese Room introduced the symbol grounding problem as a thought experiment that inspired Stevan Harnad to define his version of the problem \cite{harnad:1990}. Although controversial, the Chinese room experiment showed that there are nontrivial problems arising when one builds a cognitive robot that should be able to acquire a meaningful language system. The arguments presented against the Chinese room are the core of the argument why robots can ground language. As shall become clear, there's more to language than just symbol manipulation according to some rules.

\subsection{Symbol Grounding: Philosophical or Technical?}

\index{symbol grounding problem}
Although it might seem very philosophical up to now, this thesis in no way tries to solve the philosophical problem of what is meaning. In fact there is no attempt being made in solving any philosophical problem. The only thing that is done here is to translate a philosophical problem into a technical problem, which will be tackled in this work. The solution to the technical problem could then be the meat that the philosopher uses to solve their problem.

The problem that is tried to be solved is roughly this: how can internal symbol structures become meaningful relations to external physical structures? \citeN{harnad:1990} argued that this problem could be solved by invoking neural networks. In his article Harnad recognizes three main tasks of grounding symbols:

\begin{enumerate}
\index{symbol grounding problem!iconization}
\item {\bf Iconization} Analog signals need to be transformed to {\em iconic representation} (or icons).
\index{symbol grounding problem!discrimination}
\item {\bf Discrimination} ``[The ability] to judge whether two inputs are the same or different, and, if different, how different they are.''
\index{symbol grounding problem!identification}\index{invariance}
\item {\bf Identification} ``[The ability] to be able to assign a unique (usually arbitrary) response -- a `name' -- to a class of inputs, treating them all as equivalent or {\em invariant} in some respect.'' \cite[my italics]{harnad:1990}
\end{enumerate}

\n
According to Harnad, iconic representations are non-symbolic, but in this thesis we will assume that they are (sub)symbolic. They are represented as symbolic structures, but have no meaning.

So, what is the problem? Analog signals can be classified simply with meaningless symbolic structures. The ability to discriminate is easy to implement just by comparing two different sensory inputs (this is still cf. Harnad). The ability to identify requires to find {\em invariant} properties of objects, events and state of affairs. Since finding distinctions is rather easy, the big problem in grounding actually reduces to identifying

\begin{quote}
invariant features of the sensory projection that will reliably distinguish a member of  a category from any nonmembers with which it could be confused. \cite{harnad:1990}
\end{quote}

Although people might disagree, for the roboticists this is not more than a technical problem. The question is whether or not there exist real invariant features of a category in the world. This probably could be doubted quite seriously (see e.g. \cite{harnad:1993}). Stevan Harnad proposes that the SGP for a robot could possibly be solved by invoking (hybrid) connectionist models with a serious interface to the outside world in the form of {\em transducers} (or sensors) \cite{harnad:1993}. For the time being it is assumed that there are invariant properties in the world and it will be shown that these {\em invariants} can be found if an embodied agent is equipped with the right physical body and control. The latter inference is in line with the {\em physical grounding hypothesis} \cite{brooks:1990}, which will be discussed below.

\index{physical!grounding hypothesis}
\subsection{Physical Grounding Hypothesis}\label{s:theory:pgh}

An alternative approach to symbol grounding is physical grounding. In his article {\em Elephants Don't Play Chess} Rodney Brooks (1990) proposed the physical grounding hypothesis (PGH) as an alternative to the physical symbol system hypothesis (PSSH). 

\index{physical!symbol system}
\index{brooks}

\begin{quote}
The physical grounding hypothesis states that to build a system that is intelligent it is necessary to have its representations grounded in the physical world. \cite{brooks:1990}
\end{quote}

The advantage of the PGH over PSSH is that the system (or agent) is (by definition) directly coupled to the real world through its set of sensors and actuators. 

\begin{quote}
Typed input and output are no longer of interest. They are not physically grounded. \cite{brooks:1990}
\end{quote}

In Brooks' approach symbols are not a necessary condition for intelligent behavior anymore \cite{brooks:1990,brooks:1991}. Intelligent behavior can emerge from a set of simple couplings of an agent's sensors with its actuators\footnote{Note that this approach does not invoke connectionist models.}, as is also shown in e.g. \cite{steelsbrooks:1993,steels:1994,steels:1996c}. An example is {\em wall following}. Suppose a robot has two simple behaviors: 1) the tendency to move towards the wall and 2) the tendency to move away from the wall. If the robot incorporates both behaviors at once with equal strength, then the resulting {\em emergent} behavior is wall following. The argument that Brooks uses to propose the PGH is that

\begin{quote}
[evolution] suggests that problem solving behavior, language, expert knowledge and application, and reason, are all rather simple once the essence of being and reacting are available. That essence is the ability to move around in a dynamic environment, sensing the surroundings to a degree sufficient to achieve the necessary maintenance of life and reproduction. \cite{brooks:1990}
\end{quote}

\n
The rapid evolution is illustrated in figure \ref{f:theory:evolution}. Furthermore, Brooks uses an argument of the rapid evolution of human intelligence as opposed to the duration of the evolution of life on earth.

\begin{quote}
[O]nce evolution had symbols and representations things started moving rather quickly. Thus symbols are the key invention ... Without a carefully built physical grounding any symbolic representation will be mismatched to its sensors and actuators. \cite{brooks:1990}
\end{quote}

\index{physical!grounding hypothesis}
\index{brooks}
\index{subsumption architecture}
\index{situated!embodiment}

To explore this point of the PGH, Brooks and his co-workers at the MIT AI Lab developed a software architecture called the {\em subsumption architecture} \cite{brooks:1986}. This architecture is designed to connect a robot's sensors to its actuators so that it {\em embeds the robot correctly in the world} \cite{brooks:1990}.  The point made by Brooks is that intelligence can emerge from an agent's physical interactions with the world. So, the robot that needs to be built should be both {\em embodied} and {\em situated}. 



\subsection{Physical Symbol Grounding}
\index{physical!symbol grounding problem}

This section discussed the symbol grounding problem. Starting with Newell and Simon's PSS, via Searle's Chinese Room experiment and Harnad's symbol grounding the discussion arrived at the physical grounding hypothesis of Brooks. In Brooks' work the idea of symbols have been washed away (at least to the background), intelligence could be physically grounded from sensorimotor couplings without having symbolic representations \cite{brooks:1991}.

\index{physical!grounding hypothesis}
\index{symbol grounding problem}
Already numerous systems have been physically grounded, see e.g. \cite{brooks:1990,steels:1994,barnesetal:1997,KroBunVlaMot99,taninolfi:1998,berthouzekuniyoshi:1998,pfeiferscheier:1999,billard:1997a,rosenstein:1998a,yancostein} and many more. However, a lot of of these systems do not ground symbolic structures; they merely ground simple physical behaviors. 

\index{situated!cognition}
However, since the aim here is to ground conceptual structures that can be used in a communication system, they have to be labeled. Hence, these structures need to be accessed, so they could be termed {\em symbols} in the sense of Newell and Simon. In his book {\em Situated Cognition} \cite{clancey:1997} argued for a physically grounded symbol system, where the classical definition of symbols should be reconsidered. Clancey proposed structural couplings as a candidate.

Only a few physically grounded systems mentioned above grounded symbolic structures in the modern sense. This is for instance the work of \cite{billard:1997a,rosenstein:1998a,yancostein} in relation to language and of \cite{taninolfi:1998,berthouzekuniyoshi:1998} in relation to attentional tasks during, e.g. navigation. Some of these systems will be discussed briefly in section \ref{s:theory:otherwork}. 

This thesis discusses recent work done at the VUB AI Lab that also grounds symbolic structures physically in relation to language \cite{steelsvogt:1997} and \cite{belpaeme:1998}. Like the other physically grounded symbol systems this is a Brooksian approach to grounding but it invokes symbolic structures\footnote{Although some of the other systems mentioned {\em do} invoke connectionist models \cite{billard:1997a,taninolfi:1998}, they might be viewed as physically grounded in the Brooksian sense.}. The problem that is tried to be solved here is what might be called the {\em physical symbol grounding problem}. This problem shall not be treated philosophically but technically. It will be shown that the quality of the physically grounded interaction is essential to the quality of the symbol grounding. This is in line with Brooks' observation that a.o. language is 

\begin{quote}
rather easy once the essence of being and reacting are available. \cite{brooks:1990}
\end{quote}

Now that it is clear that the physical symbol grounding problem in this work is considered to be a technical problem, the question rises how it is solved? Before this question can be answered, it is good to discuss some biological principles on which the model is based.

\subsection{Prototypes}

\index{prototype}
\index{Rosch, Eleanor}
The representation of the categories\footnote{The categories in the work presented here are assumed to be symbolic structures. In the text the term category may be interchanged with the terms of {\em concept} or {\em meaning}. As will become clear, these terms will be defined differently. It should be clear from the context when such a term is used in the general sense or in the specific sense.} that the robots ground will be based on the prototype theory of \cite{rosch:1976}. According to this theory things that exist in the world are categorized in prototypes. A prototype could be characterized by the most typical exemplar of a class. However non-typical exemplars can be categorized within the same class when it shows many similarities.

So, for instance the color red has different shades of red. Yet, there appears to be a typical color of red, like the red of a Ferrari. Nevertheless, humans are perfectly well capable of categorizing the different shades of red, like there is the red of a radish or the red of the sky in sunset. The prototypical color red (the red of the Ferrari) is called the {\em central} member of the category red. Most categories have a sort of gradience from the central member outward. Where some categories have clear boundaries, others may have them not \cite{lakoff:1987}.

\index{family resemblance}
\index{Wittgenstein, Ludwig}
\index{prototype!effect}
\citeN{rosch:1978} showed empirically that categorization typically showed a {\em prototype effect}. Subjects trying to categorize different perceptions, were faster in categorizing the central members of a category than the non-central members. The prototype theory is in line with the notion of {\em family resemblance} \cite{wittgenstein:1958}. Seemingless completely different things can be categorized with a category despite the fact that the central member of the category has nothing in common with the other member. This can be explained with the notion that the other member has similarities with other non-central members of the category, which in turn have things in common with the central member. 

The notion of a prototype is widely accepted among the cognitive scientist. To quote Paul M. \citeN[p. 122]{p.m.churchland:1989}:

\begin{quote}
``Various theorist have independently found motive to introduce such a notion in a number of cognitive fields: they have been called `paradigms' and `exemplars' in the philosophy of science \cite{kuhn:1962}, `stereotypes' in semantics \cite{putnam:1975}, `frames' \cite{minsky:1981} and `scripts' \cite{schank:1977} in AI research, and finally `prototypes' in psychology \cite{rosch:1976} and linguistics \cite{lakoff:1987}.
\end{quote}

\index{prototype}Also in the field of connectionism \cite{a.clark:1993} and neurosciences \cite{p.m.churchland:1989} the notion of prototypes is accepted in a wide field. 

\begin{quote}
The natural mechanisms of connectionist learning and superpositional storage immediately yield a system which will extract the {\em statistical central tendency} of the exemplars.\cite[p. 16]{a.clark:1993}
\end{quote}

In the representation of the prototypical categories as exploited in this thesis, the prototypes will also extract the statistical central tendency of the exemplars that are conceptualized successfully (see chapter \ref{ch:cm}). The method results in the fact that the central values of the prototypes need not to be identical to any real observation, which is conform the idea of prototypes \cite{lakoff:1987}.


\section{Selectionism}\label{s:theory:select}

\index{selectionism}
Part of the essence of the theory as proposed by Luc Steels is inspired biologically \cite{steels:1994a,steels:1996a}. The idea is that agents acquire concepts and language through generation and selection of elements of the ontology and lexicon. As will become clear the acquisition mechanisms of the ontology are pretty similar for the categories, concepts and lexicon; all structures are acquired through {\em selectionism}. So, what is selectionism?

\begin{figure}
\centerline{\psfig{figure=theory//evol.eps,width=11.4cm}}
\caption{\index{cultural evolution}(a) The evolutionary time-scale of life and cognitive abilities on earth. After the entrance of the Great Apes, evolution of man went so fast, that it cannot be shown on the same plot, unless it is shown in logarithmic scale, see (b). It appears from the plot that cultural evolution works much faster than biological evolution. Time-scale is adapted from (Brooks 1990).}
\label{f:theory:evolution}
\end{figure}

Selectionism in a traditional Darwinian sense is a principle that is used to select fit organisms of a population so that the most fittest organisms can reproduce to increase the fitness of the population \cite{darwin:1968}. This Darwinian tradition is still one of the pillars of biology and biological evolution. It is also one of the pillars in the biologically inspired field of the Artificial Life (ALife), and Genetic Programming (GP) in particular. It is simply the essence of genetic evolution. But genetic evolution is slow since it works over generations of a population, see figure \ref{f:theory:evolution}. 

\subsection{Cultural Evolution}

\index{cultural evolution}
In the figure we see how fast evolution of man and its culture is opposed to the biological evolution (until the arrival of Homo Sapiens). So, it seems that cultural evolution works at a different time scale than biological evolution.\footnote{This is again a similar argument as has been made by \citeN{brooks:1990}}. This is not totally surprising, since culture spreads through another medium than biological cells. 

In his book 'The Selfish Gene' \cite{dawkins:1976} argues that cultural evolution can be compared with biological evolution if one considers {\em memes} for genes. A meme is what Dawkins called an {\em idea}, like for instance the concept of a wheel. According to Dawkins memes may evolve under natural selection. Memes are spread by agents\footnote{It is assumed that only higher organisms can spread ideas, although one could argue that lower organisms could also spread ideas albeit it 'lower' or simple ideas.} and multiple generations of memes can be propagated within the life time of one generation of the agent (e.g. humans). The propagation of memes can be accomplished by for instance using language, but also by imitation of behavior as primates do when they learn to wash their food in the sea before eating\footnote{For instance Japanese macaque monkeys wash their food in water before they eat it. It has been observed that young macaque monkeys learn this behavior by observation \cite{byrne:1995}} or use tools to find food.

Memes can mutate, either by wrongly learning a behavior through observation, or simply because physical conditions change (e.g. when washing food while there are high or low waves), or an agent can improve (or even get worse) on a particular (cultural) behavior, like washing food. Of course the changed behavior can be acquired by another agent, thus memes can be spread mutated and a huge variety of memes become available in the 'meme-soup'. Because of our (biological) tendency to use the most {\em effective} strategies to survive\footnote{The essence of natural selection} (or maybe nowadays even to have fun), the most effective memes are proned to be selected.
\index{cultural evolution}

\subsection{Lifetime Neurological Evolution}\label{s:theory:tngs}

\index{Theory of Neuronal Group Selection|see{Neural Darwinism}}
\index{Neural Darwinism|(}
Similar, though at a physical level, one can translate selectionist evolution to neural development \cite{edelman:1987,changeux:1984}. Inspired by his Noble-prize winning research on selectionist approaches towards immunology, Gerald Edelman proposed the {\em Theory of Neuronal Group Selection} (TNGS) \cite{edelman:1987}. In this theory neurons are viewed as individuals that constitute populations (or groups). Those neurons that have close connections with each other and that seem to be active simultaneously when certain patterns are presented are called a groups. These groups, Edelman hypothesized, are organized during the development of an individual by a Darwinian-like selectionist process.

The TNGS has three tenets, which

\begin{quote}
are concerned with how the anatomy of the brain is first set up during development, how patterns of responses are then selected from this anatomy during experience, and how reentry, a process of signaling between the resulting maps of the brain give rise to behaviorally important functions. \cite[p. 83]{edelman:1992}
\end{quote}

Perceptual categorization is in the TNGS a process in which (reentrant) classification couplings between functionally different local maps are selectively activated. The perceptual category is a coupling of the sensory stimulus with some motor activity causing the agent's behavior. If the behavior is {\em appropriate} from the viewpoint of the agent, then the synaptic connections of the active structures and adjacent neurons are strengthened thus increasing the strength of the neuronal group. The theory has been tested on several simulated robots of the Darwin series, see e.g. \cite{reekeetal:1989}.

A similar notion of couplings has also been made by Maturana and Varela \cite{maturanavarela:1992}. According to them all there is to behavior of any organism can be explained by the structural couplings organisms have with their environment.

It outside the scope of this dissertation to explain the TNGS and other biological theories in detail. If one is interested, see e.g. \cite{johnson:1997} for an account on developmental cognitive neuroscience. It is sufficient to notice that there are biological theories that propose a selectionist functioning of the brain. Furthermore, selectionism may work at different functional levels in biology {\bf and} in social behavior, which is an indication that it may be an universal principle of development.

\index{Neural Darwinism|)}
\subsection{Self-Organization}
\index{self-organization|(}

One key feature of selectionism (and indeed of much more biological and other processes) that deserves special attention is {\em self-organization}. Self-organization is a principle that is very common in dynamical systems. It is a principle in which a system evolves towards a certain ordered structure as a spontaneous result of its own behavior. Self-organization may be influenced by the system's environment, although one might view the (immediate) environment (or {\em niche}) as part of the system as well. The latter standpoint is the {\em ecological} view of behavior.

Self-organization in (complex) dynamical systems (biological and others) has been reported extensively in the literature, see e.g. \cite{prigogine,maynard-smith:1995}. It has been observed in for instance biology, physics, chemistry, social sciences, economics and recently there is a tendency to investigate self-organization in the cognitive sciences as well, see e.g. \cite{edelman:1992,smiththelen:1993}. These cognitive scientist

\index{embodiment}\index{feedback}
\begin{quote}
recognize that learning is a process that occurs only in systems that are ``environmentally embedded, corporeally embodied, and neurally entrained'' by feedback. \cite[p. 738]{wilsonkeil:1999}
\end{quote}

Thus learning is the result of self-organization and the interaction of the (physical embodied) agent with its environment. A term that is often used in relation (or intertwined) with self-organization is {\em emergence}. Some complex structures (or behaviors) emergence from the interaction of several other (often much more simpler) structures. The way that (biological) evolution organized its structures is the result of ongoing adaptation of the organisms that constitute the system. 

\index{emergence}\index{dissipative structure}\index{attractor}

The structures that emerge are sometimes called {\em dissipative structures} \cite{prigogine} or {\em attractors}. An example of a dissipative structure is path formation in ants. The approach that is taken here is that the ontologies and lexicons that agents acquire are attractors of the complex dynamical system \cite{steels:1996a}. 

One of the key issues in complex dynamical systems is that starting from a set of (well-defined) boundary-conditions and following a nonempty set of (physical) mechanisms a system evolves towards an {\em attractor}. The attractor forms a region in the {\em phase space} towards which the system evolves if its state is close enough to the attractor. 

How can all this be interpreted intuitively in terms of language? Suppose that we have a system in which there are agents that have the ability to communicate with each other under the restriction of particular (physical) laws. And suppose further that these agents initially are in a particular state that forms a boundary of the attractor. Then the system may evolve to a structure which might be called {\em language}. It is beyond the scope of this thesis to go into the theory of complex dynamics in more details. Edwin \citeN{dejong:2000} made an elegant analysis of the attractor dynamics of language. The main task of this dissertation is to develop such a system for two mobile robots with a simple physical body.

\index{self-organization|)}


\section{Early Child Language}\label{s:theory:acquisition}

\index{language!acquisition|(}
Children acquire a full language in their first 14 years or so. Already at the age of six, they are well capable of producing well formed sentences in many different domains. What is most interesting however for the purpose of this thesis is the early lexicon development. This section will sketch some of the issues raised in the theory about early lexicon acquisition.

\p
Early lexicon acquisition is characterized in two stages: single-word utterances and two-word utterances. The first stage is observed mainly between the age 1;0 and age 1;6 (i.e. between ages 1 and 1 year and 6 months). The child starts to combine 2 or more words from about age 1;6. Many studies have been done to monitor a child's lexical growth. \citeN{nelson:1973} studied how many words a young child spontaneously produced between the ages of 1 and 2. Eighteen children were monitored and Nelson found that the average age at which the children had acquired 10 words was 15 months, 50 words were acquired by 20 months and the average vocabulary size at 24 months was 186 words. A larger study (involving 1,789 children) conducted by \citeN{fensonetal:1993} using a different methodology found that at the average age of 13 months, children acquired the first 10 words. By 17 months of age, 50 words were acquired and at 24 months they had acquired 310 words\footnote{The data of the two studies were taken from \cite{barrett:1995}.}.

\n
For the purpose of this work it is interesting to know what type of non-linguistic information children have access to when acquiring new words. It is especially interesting how and to what children focus their attention when acquiring new words, and if so, how feedback is provided. 

The communication acts of young children is mostly about the 'here and now'. So, the context is usually visually available to the child \cite{clarkclark:1977}. First communicative exchanges start by using gestures, which are initiated by adults.

\index{joint attention}
\begin{quote}
From seven or eight months on, adults start to use pointing gestures to direct the infants' attention in addition to showing and offering new or potentially interesting objects. ... [W]ithin three months or so, infants begin to reciprocate: they start using pointing gestures to pick things out ... When infants show, offer, and point things out for a ``listener'', they are using a primitive form of communication. \cite[pp. 312-313]{clarkclark:1977}
\end{quote}

By the age of one children know that pointing is used to attract the attention to something. It is widely assumed that children acquire new words when their parents point to and name objects:

\index{pointing}
\begin{quote}
Western middle-class parents often point to and name objects for young children in what is known as the {\em ostensive naming context} or the {\em original word game} \cite{brown:1958}. There is a widespread assumption among lexical acquisition researchers that this learning context is representative of lexical acquisition in general. Virtually all of the theoretical analysis and empirical research in the constraints approach to lexical acquisition, for example, address noun learning in the ostensive context \cite{markman:1989}.\cite[p. 639]{tomasellobarton:1994}
\end{quote}

Pointing is naturally used to draw the child's attention on the introduction of a novel word. Pointing, however, is not always used to teach children new words.

\begin{quote}
...[T]he nonostensive contexts ... are not strange or infrequent learning contexts in the lives of beginning language learners. ... Children in many cultures, and many children in Western cultures (e.g. working-class children and children growing up in multichild homes), experience their language in a variety of pragmatic context, not just those in which someone points and names things for them (cf. \cite{bartontomasello:1994,schieffelinochs:1989}). \cite[p. 649]{tomasellobarton:1994}
\end{quote}
\index{pointing}\index{joint attention}

Western-middle class parents probably assume that their children learn language faster when provided with ostensive cues than without. 

\citeN{tomasellobarton:1994} have conducted an experiment with 2-years-old children giving them both ostensive and nonostensive cues while presenting them a novel non-English word ({\bf toma}) together with one or more toys for which the children had not yet acquired a word. The nonostensive cues were provided in a test group and the ostensive cues were provided in the control group. Tomasello and Barton found that the children could learn the new word equally well when provided with ostensive as nonostensive cues. It is important to note that the children received both positive and negative feedback during the learning and testing phase, when they tried to match the given non-word and an object they could choose from.

\index{feedback}
The above results, however, do not necessarily mean that there are no other cues in a child's acquisition phase to draw a child's joint attention. There appear to exist much more different types of cues, like the adult's gaze to an object or other social-pragmatic referencing, see e.g. \cite{baldwin:1993,tomasello:1992}. When it is not there immediately, providing (behavioral) feedback appears necessary in the child's early language learning \cite{tomasellobarton:1994}. So, although not necessary, the role of joint attention is indisputable. 

\citeN{barrett:1995} argues that from studies like in \cite{schafferetal:1983,harrisetal:1983,harrisetal:1984}

\begin{quote}
... have revealed that, during social-interactive episodes involving mother and young child, the mother tends to monitor the child's focus of attention very carefully, and to time her utterances to coincide with moments in the interaction when the relevant referent (either an object or an action) already occupies the child's focus of attention. \cite[p. 390]{barrett:1995}
\end{quote}

Barrett discusses that there is clear evidence for a positive relation between the time mothers spend in this kind of joint attention and the child's later vocabulary size \cite{tomasellotodd:1983,tomaselloetal:1986}. Tomasello and co-workers found however that when the attention of the child was drawn explicitly to a referent, this relation was negatively correlated with the proportion of object names in the child's vocabulary.

So, it appears that when learning their first words, children learn best when there is a co-occurrence of a certain utterance and the children's focus of attention towards the referent of the utterance's meaning. This relation  is positive if the attention of the child is spontaneous rather than when it is drawn by someone else. Besides in the psycholinguistic literature, the role of joint attention has also been stressed extensively in the evolution of language society, see e.g. \cite{dessallesghadakpour:2000}.

\index{joint attention}
\index{feedback}
\index{structural coupling}
\p
The literature on situated cognition stresses the importance of feedback as a structural coupling between an organism and its environment \cite{clancey:1997,maturanavarela:1992}. The way this feedback is provided is an important issue of debate in child language acquisition (see e.g. \cite{bowerman:1988}). Is the feedback provided {\em positive} or {\em negative}, {\em explicit} or {\em implicit}? Questions that cannot be answered immediately and need some investigation. The nativist approach assumes that the child can get feedback from its environment when testing hypotheses. As \citeN{bowerman:1988} puts it when discussing the ideas of \citeN{braine:1971}:

\begin{quote}
[For the nativist approach] hypothesis testing requires feedback about the correctness of predictions, pointed out Braine. In particular, it requires evidence not only about what {\em is} an instance of is to be learned [...], but also what is {\em not} an instance.\cite[p. 74]{bowerman:1988}
\end{quote}

Although the argument applied directly to the nativist theory and to the acquisition of grammar, it also applies to this work, since the proposed model uses negative evidence as feedback as well. And

\begin{quote}
[a]fter reviewing the available data (e.g. \cite{brownhanlon:1970}), Braine concluded that negative evidence is rare in the input to children; moreover, children appear to be relative impervious to what little correction they do receive. \cite[p. 75]{bowerman:1988}
\end{quote}

\index{feedback!no negative feedback evidence}
This problem has become known as the {\em no negative feedback evidence} and has been discussed a lot in the linguistic literature (e.g. \cite{pinker:1994,hawkins:1988}). The problem however, is very disputable and it very much underestimates the children's ability to interpret the success of a language game. Children do try to get the listener's acknowledgment on the topic when communicating about something \cite{clarkclark:1977}. Furthermore, there is evidence in favor for the availability of explicit feedback is found for instance in \cite{brownhanlon:1970,hirsch-paseketal:1984,demetrasetal:1986}, although it is provided infrequently. Negative feedback has been found to occur by \cite{moerk:1983,hirsch-paseketal:1984,demetrasetal:1986}, mainly in repetitions. For instance the phrase ``no this is not a toma'' is a repetition. Feedback is mostly provided implicitly in a conversation. For instance when a child hands the wrong item when asked for something.  

This type of implicit (or explicit?) feedback can well be negative, and children are most probably well capable of interpreting the feedback. Both negative and positive feedback has been provided by the experimenters in research of \cite{tomasellobarton:1994} as well. When the experimenters or child interacted with the intended object after the word has been expressed, they would gee cheerfully. When they encountered an unintended object, disapprovement was shown.

\p
Summarizing this section, children usually communicate about the things that are `here and now'. They mostly acquire a new word best when their attention is focused spontaneously on the topic, although providing an ostensive context (i.e. combining naming with pointing) is not a necessary trigger. It seems that feedback is mostly provided implicitly. Although there is a large linguistic community favoring the no negative feedback evidence, there are studies showing that this is very much debatable.

\index{language!acquisition|)}

\section{Related Work}\label{s:theory:otherwork}

Many attempts in AI have been made solve the grounding problem. Furthermore, there is recently a lot of computational research in the area of the origins and evolution of language. It will be out of the scope of this work to discuss them all, but in this section most closely related work will be discussed briefly. 

First, there are several robotics implementation of the grounding problem, but most interesting are those related to language learning and origins like \cite{yancostein,billard:1997a,rosenstein:1998a}. Some simulated versions are also of interest, namely the work of \cite{cangelosiparisi:1998,cangelosi:1998}. Secondly, there is quite some work in adaptive and selectionist approaches to the evolution of language which is of interest, for instance \cite{wernerdyer:1991,kirby:1997,oliphant:1998}. Finally, and most closely related is the work done at the Sony CSL Lab in Paris and at the VUB AI Lab, in particular the work on the Talking Heads \cite{belpaeme:1998,kaplan:2000,steels:2000}.

The remainder of this chapter briefly discusses some of these related research. Their approaches will be presented and main differences will be noticed. The related work will be of interest when discussing the results in chapter \ref{ch:disc}.

\subsection{Symbol Grounding on Robots}
\index{symbol grounding problem|(}

It is a well known fact among roboticists that grounding invariant categories is a very hard problem, see e.g. \cite{pfeiferscheier:1999}. And although most robotics applications face the grounding problem, most interesting to this work is the work related to language grounding.

\p
The first interesting research was carried out by \citeN{yancostein} at the MIT AI Lab. Yanco and Stein developed a troupe of robots that could learn to associate certain actions with a pre-defined set of words. One robot would decide what action is to be taken and communicates a relating signal to the other robot(s). The learning strategy they used was reinforcement learning where the feedback in their task completion was provided by a human instructor. If both robots performed the same task, a positive reinforcement was give, and when both robots did not, the feedback consisted of a negative reinforcement.

The research was primarily focussed on the learning associations between word and meaning on physical robots. No real solution was attempted to solve the grounding problem and only a limited set of word-meaning associations were pre-defined. In addition, the robots learned by means of supervised learning with a human instructor. \citeN{yancostein} showed however, that a group of robots could converge in learning such a communication system.

\p
In \citeN{billard:1997a} two robots grounded a language by means of imitation. The experiments consisted of a teacher robot, which had a pre-defined communication system, and a student robot, which had to learn the teacher's language by following it. The learning mechanism was provided by an associative neural network architecture called DRAMA. This NN learned associations between communication signals and sensorimotor couplings. Feedback was provided by the student's evaluation if it was still following the teacher.

So, the language was grounded by the student using this neural network architecture, which is derived from Wilshaw networks. Associations for the teacher robot was pre-defined in their couplings and weights. The student could learn a limited amount of associations of actions and perceptions very rapidly \cite{billard:1998}.

\p
\citeN{rosenstein:1998a} developed a robot that could ground time series by using the so-called method of delays, which is drawn from the theory of non-linear dynamics. The time series the robots produce by interacting in their environment are categorized by comparing their delay vectors, which is a low-dimensional reconstruction of the original time series, with a set of prototypes. The concepts the robots thus ground could be used for grounding word-meanings \cite{rosenstein:1998b}.

The method proposed by \citeN{rosenstein:1998a} has been incorporated in a language experiment where two robots play follow me games to construct an ontology and lexicon to communicate their actions \cite{vogt:1999a,vogt:2000}. This was a preliminary experiment, but the results appear to be promising.

A similar experiment on language acquisition on mobile robots has been done by the same group of Rosenstein and Cohen at the University of Massachusetts \cite{oatesetal:1999}. The time series of a robot's actions are categorized using a clustering method for distinctions \cite{oates:1999}. Similarities between observed time series and prototypes are calculated using dynamical time warping. The thus conceptualized time series are then analyzed in terms of human linguistic interactions, who describe what they see when watching a movie of the robot operating \cite{oatesetal:1999}.

\subsection{Computational Approaches to the Origins and Evolution of Language}

In the past decade an increasing research on the origins and evolution of language has emerged where computer simulations were the primary tool of the researchers. Despite the fact that when using a {\em simulation} researchers only {\em calculate} the validity of their model, such simulations can come up with interesting results. A brief overview of most related work is discussed below.

\subsubsection{Grounding Languages}

Most related work in the simulation of grounded communication is done by Angelo Cangelosi \cite{cangelosi:1998}. In his work an ecology of edible and non-edible mushrooms has been created. Agents that are provided with neural networks learn to categorize the mushrooms from `visible' features into the categories of edible and non-edible mushrooms. As appeared from the results, communication helps the agents to improve their categorization abilities \cite{cangelosiharnad:2000}.

Other interesting work is done by Terry \citeN{regier:1995}, who grounded spatial relations for communication, again using a neural network architecture. Also other grounded, but simulated experiments are done at the Neural Theory of Language group of the University of California in Berkeley, e.g. \cite{bailyetal:1998}. NTL focussed on the embodiment in learning languages using neural networks.

Additional work can be found in {\em The Grounding of Word Meaning: Data and Models} \cite{gasser:1998}, the proceedings of a joint workshop on the grounding of word meaning of the AAAI and Cognitive Science Society. In these proceedings, grounding of word meaning is discussed among computer scientists, linguistics and psychologist. 

\p
A problem with simulations of grounding is that the problem cannot be solved in principle, because the agents that `ground' symbols do not do so in the {\em real world}. However, these simulations are useful in that they can learn us more about how categories and words could be grounded. In this light it is interesting to note the first simulations in the current framework on grounding done by Luc \citeN{steels:1996b} and the work done by Edwin \citeN{dejong:1999} of the VUB AI Lab, which will be discussed later.
\index{symbol grounding problem|)}

\subsubsection{Evolutionary Linguistics}
\index{evolutionary linguistics|(}
\index{Oliphant, Mike|(}

In the last decade many computational approaches to the evolution of languages emerged. Many of these are concentrated on the paradigm in which language is viewed as an adaptive emergent phenomena. It goes beyond the scope of this paper to discuss all the research, but there is one research that is of particular interest for this thesis, namely the work of Mike Oliphant \cite{oliphant:1997,oliphant:1998,oliphant:2000}.

Oliphant simulates the learning of a symbolic communication system in which a fixed number of signals are matched with a fixed number of meanings. The number of signals that can be learned is equal to the number of meanings. Such a coherent mapping is called a Saussurean sign \cite{saussure:1959} and is the idealization of language. The learning paradigm of Oliphant is an {\em observational} one and he uses an associative network incorporating Hebbian learning. With observational is meant that the agents during a language game have access to both the linguistic signal and its meaning.

As long as the communicating agents are aware of the meaning they are signaling, the Saussurean sign can be learned \cite{oliphant:1997,oliphant:2000}. The awareness of the meaning meant by the signal should be acquired by observation in the environment. Oliphant further argues that reinforcement types of learning as used by \cite{yancostein,steels:1996a} are not necessary and unlikely (see also the discussion about the no negative feedback evidence in section \ref{s:theory:acquisition}). Although he does not say they are not a possible source of language learning \cite{oliphant:2000}.

The claim Oliphant makes has implications on why only humans can learn language. According to \citeN{oliphant:1998}, animals have difficulty in matching a signal to a meaning when it is not an innate feature of the animal. Although this is arguable (Oliphant refers here to e.g. \cite{gardners:1969,premack:1971}), he observes the fact that in these animal learning the communication is explicitly taught by the researchers. The observation argument of Oliphant is in line with the joint attention paradigm, see section \ref{s:theory:acquisition}.

\index{evolutionary linguistics|)}
\index{Oliphant, Mike|)}

\subsection{Sony CSL Paris and the VUB AI Lab}\label{s:theory:th}

\index{Steels, Luc|(}

As mentioned in chapter \ref{ch:intro}, this research is part of the research done at the Computer Science Laboratory of Sony in Paris and at the Artificial Intelligence Laboratory of the Free University of Brussels, both directed by Luc Steels. The two laboratories have a large variety of researches concentrating on the origins and evolution of language. 

Investigation is done on both simulations and grounded robots. It focuses on the origins of sound systems, in particular in the field of phonetics \cite{deboer:1997,deboer:1999,oudeyer:1999}, the origins of meaning \cite{steels:1996b,steelsvogt:1997,dejongvogt:1998,vogt:1998a,dejong:1999}, the emergence of lexicons \cite{steels:1996a,steelskaplan:1998,kaplan:2000,vogt:1998b,vanlooveren:1999}, the origins of communication \cite{dejong:1999c,dejong:2000} and the emergence of syntax \cite{steels:2000a}. These subjects investigate various aspects of language like stochasticity \cite{steelskaplan:1998,kaplan:2000}, dynamic language change \cite{steels:1997c,steelsmcintyre:1999,deboervogt:1999}, multi-word utterances \cite{vanlooveren:1999}, situation concepts \cite{dejong:99b} and grounding \cite{belpaeme:1998,steelsvogt:1997,steels:2000,kaplan:2000}.

\p
Bart de Boer of the VUB AI Lab has shown how agents can develop a human vowel system through self-organization \cite{deboer:1997,deboer:1999}. These agents were modeled with a human like vocal tract and auditory system. Through cultural interactions and imitations the agents learned vowel systems as they are found prominently among human languages. 

\index{Talking Heads|(}
\p
First in simulations \cite{steels:1996a,steels:1996b} and later in grounded experiments on mobile robots \cite{steelsvogt:1997,vogt:1998a,vogt:1998b,dejongvogt:1998} and on the Talking Heads \cite{belpaeme:1998,kaplan:2000,steels:2000} the emergence of meaning and lexicons have been investigated. Since the mobile robots experiment is the issue of the current thesis, the other work will be discussed briefly here. 

The simulations began fairly simple by assuming a relative perfect world \cite{steels:1996a,steels:1996b}. Software agents played naming and discrimination games to create lexicons and meaning. The lexicons were formed to name predefined meanings and the meanings were created to discriminate predefined visual features. In later experiments more complexity was added to the experiments. From findings of the mobile robots experiments \cite{vogt:1998a} it was found that the ideal assumptions of the naming game, for instance, considering the topic to be known by the hearer, were not satisfied. Therefore a more  naming game was developed that could handle noise of the environment \cite{steelskaplan:1998}. 

For coupling the discrimination game to the naming game, which first has been done in \cite{steelsvogt:1997}, a new software environment was created: the GEOM world \cite{steels:2000}. The GEOM world consisted of an environment in which geometric figures could be categorized through the discrimination game. The resulting categories could then be lexicalized using the naming game. The Talking Heads is also situated in a world of geometrical shapes that are pasted on a white board the cameras of the heads look at (figure \ref{f:theory:talkingheads}).

\begin{figure}
\centering
\psfig{figure=theory//th.eps,width=11.6cm}
\caption{The Talking Heads as it is installed at Sony CSL Paris.}
\label{f:theory:talkingheads}
\end{figure}

The Talking Heads consists of a couple of installations that are distributed around the world. Permanent installations exist currently in Paris at the Sony CSL, in Brussels at the VUB AI Lab, in Amsterdam at the Intelligent Autonomous Systems laboratory of the University of Amsterdam. Temporal installations have been operational in Antwerp, Tokyo, Laussane, Cambridge, London and at another site in Paris. Agents can travel the world through the internet and embody themselves into a Talking Head. A Talking Head is a pan-tilt camera connected to a computer. The Talking Heads play language games with the cognitive capacities and memories that each agent has or has acquired. The language games are similar to the ones that are presented in the subsequent chapters. The main difference is that the Talking Heads do not move from their place, which the mobile robots do. The Talking Heads have cameras as their primary sensory apparatus and there are some slight differences in the cognitive capabilities as will become clear in the rest of this thesis.


\index{Talking Heads|)}
\index{Steels, Luc|)}
\index{Kaplan, Fr\'ed\'eric}
\index{AIBO}

All these experiments show similar results. Categories and word-meaning pairs can be grounded in sensorimotor control, for which (cultural) interactions, individual adaptation and self-organization are the key points of the model. A similar conclusion will be drawn at the end of this thesis. The results of the experiments on mobile robots will be compared with the Talking Heads as reported mainly in \cite{steels:2000}. Other findings based on the different variations of the model, which inspects the different influences of the model will be compared with the PhD thesis of Fr\'ed\'eric Kaplan of Sony CSL in Paris \cite{kaplan:2000}\footnote{Currently Fr\'ed\'eric Kaplan is working on human-machine interaction on the AIBO robot that looks like a dog and which has been developed by Sony CSL in Tokyo. Naturally, the AIBO learns language according to the same principles advocated by our labs.}.

The software that runs on the Talking Heads and on many of the simulations done at Sony CSL and the AI Lab is programmed in Common LISP and is brought together in a toolkit called {\em Babel}, developed mainly by Angus McIntyre of Sony CSL \cite{mcintyre:1998}. The software for the mobile robots is written in C/C++ and PDL (a special purpose programming language for behavior-based robotics, see next chapter).

\index{De Jong, Edwin}
\p
A last set of experiments that will be brought to the reader's attention is the work done by Edwin de Jong of the VUB AI Lab. De Jong has done an outstanding experiment in which he showed that the communication systems that emerged under the conditions by which language research is done in Paris and Brussels are indeed complex dynamical systems \cite{dejong:2000}. The communication systems of his own experiments all evolved towards an attractor and he showed empirically that the system was a complex dynamical system. De Jong investigated the evolution of communication by an experiment in which agents construct a communication system about situation concepts.

One interesting finding of De Jong was that it is not necessary that agents use feedback on the outcome of their linguistic interactions to construct a coherent lexicon, provided that the robots had access to the meaning of such an interaction and lateral inhibition was assured. Hence this confirms the findings that Mike \citeN{oliphant:1998} made. Note that Oliphant used Hebbian learning to arrive at his conclusions and Hebbian learning implicitly implements lateral inhibition.

\section{Summary}

The theoretical background provided in this chapter should enable readers to follow the discussions presented in the remainder of this thesis. 

\p
The chapter started with a discussion of classical approaches to language and meaning, in particular the theories proposed by Noam Chomsky and Jerry Fodor. Both take a nativist and computational view on cognition. Empirical data however, revealed evidence against such theories. Hence alternative views should be explored

The symbol grounding problem has been introduced by another classical theory on cognition, namely Newell and Simon's physical symbol system hypothesis. This hypothesis argues that humans are physical symbol systems. In principle this is not wrong, but the symbolic structures they deal with are too static and have not grounded its meaning in the real world. According to Searle, it is impossible to build an artificial intelligence, because symbolic structures in computer programs cannot be about something that exists in the real world. However, Searle does not admit that robots can interact with the real world and hence, they might develop grounded symbol structures. Stevan Harnad identifies this problem as the symbol grounding problem. Harnad argues that a robot could ground symbols. He identified three subproblems which have to be solved: iconization, discrimination and identification. One way of defining representations of categories is using the notion of prototypes. 

Rather than a computational approach as is usual in classical approaches to cognition and AI, a selectionist framework will be explored. The transmission and evolution of the ontology and lexicon the robots develop is driven by a cultural evolution. Cultural evolution is much faster than biological evolution and many generations of an element of the evolution can occur within a lifetime. That such cognitive development can occur in the human brain has been advocated by Gerald Edelman and Jean-Pierre Changeux. One important feature of such an evolution is self-organization. In evolution, a complex dynamic system may evolve towards an attractor by self-organization. It is beyond the scope of this thesis to discuss self-organization in detail.

In the literature on child language development, a heavy debate is going on about what non-linguistic information is available to a child during language acquisition. It has been observed that there is no or little negative feedback for children and that children learn best when joint attention is established during a conversation. However, these findings are attacked with many counter-arguments.

\p
This work is not the first computational approach on the origins and evolution of language. However, it is the first approach that uses physically situated and embodied agents. The symbol grounding problem has been investigated by a numerous experiments on real robots, notably is the work of Aude Billard. 

Since Mike Oliphant also investigated the issue of joint attention and feedback, his work is also particularly interesting. With his observational learning mechanism, he showed that agents could learn a coherent communication system without the availability of feedback on the success, but provided joint attention is established.

\p
As should be clear by now, the research done at the VUB AI Lab and Sony CSL Paris is broad within the origins and evolution of language and meaning. The key ideas behind the experiments are cultural evolution and other interaction with an agent's environment, selectionist adaptation and self-organization. The remainder of this thesis will focus on these issues in much more detail.
